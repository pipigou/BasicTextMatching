{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "from collections import Counter\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "import gc\n",
    "    \n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "seed(1000)\n",
    "from tensorflow.random import set_random_seed\n",
    "set_random_seed(1000)\n",
    "\n",
    "import pandas\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "# from keras.layers import Layer\n",
    "from keras.engine import Layer\n",
    "from keras.engine import InputSpec\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Input, Embedding, Conv1D, Concatenate, Flatten, Dropout, Dense, Bidirectional, Activation\n",
    "from keras.layers import MaxPooling1D, Conv2D, Flatten, Reshape, MaxPooling2D, LSTM, Lambda, Highway, Dot, Permute\n",
    "from keras.models import Model, Sequential\n",
    "from keras.activations import softmax\n",
    "from keras.initializers import Constant, RandomNormal, RandomUniform\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns       \n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import  norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline \n",
    "\n",
    "#进行配置，使用30%的GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "# 设置session\n",
    "KTF.set_session(session )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文件路径\n",
    "ORIGINAL_FILE_PATH = \"../data/original_data/quora_duplicate_questions.tsv\"\n",
    "\n",
    "DATA_ALL_PATH = \"../data/all_data.txt\"\n",
    "\n",
    "DATA_TRAIN_PATH = \"../data/train.txt\"\n",
    "DATA_VALID_PATH = \"../data/valid.txt\"\n",
    "DATA_TEST_PATH = \"../data/test.txt\"\n",
    "\n",
    "MAPS_FILE_PATH = \"../data/maps.pkl\"\n",
    "\n",
    "# Glove文件\n",
    "WORD2VEC_PATH = \"../data/GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "# Model Path\n",
    "MODEL_PATH = \"../model/drmm_tks/drmm_tks-01.h5\"\n",
    "\n",
    "# 创建文件夹\n",
    "(file_path, file_name) = os.path.split(MODEL_PATH)\n",
    "if not os.path.exists(file_path):\n",
    "    os.mkdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(dict_path):\n",
    "    with open(dict_path, 'rb') as fr:\n",
    "        return pickle.load(fr)\n",
    "        \n",
    "def create_maps(dico):\n",
    "    item2id = dico\n",
    "    id2item = dict([val, key] for key, val in dico.items())\n",
    "    return item2id, id2item\n",
    "\n",
    "def read_file(fpath):\n",
    "    \"\"\"\n",
    "    读取文件，返回\n",
    "    \"\"\"\n",
    "    question1, question2, labels = [], [], []\n",
    "    with open(fpath, 'r', encoding=\"utf-8\") as fr:\n",
    "        for line in fr.readlines():\n",
    "            try:\n",
    "                q, k, l = line.strip().split('\\t')\n",
    "            except ValueError:\n",
    "                print(line)\n",
    "            question1.append(q)\n",
    "            question2.append(k)\n",
    "            labels.append(int(l))\n",
    "            \n",
    "    return question1, question2, labels\n",
    "\n",
    "def load_data(fpath, word_to_id, max_length=20):\n",
    "    \"\"\"\n",
    "    载入数据，并将其转化为id表示\n",
    "    \"\"\"\n",
    "    question1, question2, labels = read_file(fpath)\n",
    "    \n",
    "    q1_id, q2_id = [], []\n",
    "    for i in range(len(question1)):\n",
    "        q1= question1[i].split()\n",
    "        q2 = question2[i].split()\n",
    "        q1_id.append([word_to_id[x] for x in q1 if x in word_to_id])\n",
    "        q2_id.append([word_to_id[x] for x in q2 if x in word_to_id])\n",
    "\n",
    "    # 使用keras提供的pad_sequences来将文本pad为固定长度\n",
    "    x1_pad = keras.preprocessing.sequence.pad_sequences(q1_id, max_length, padding=\"post\")\n",
    "    x2_pad = keras.preprocessing.sequence.pad_sequences(q2_id, max_length, padding=\"post\")\n",
    "    labels = keras.utils.np_utils.to_categorical(labels)\n",
    "    \n",
    "    return x1_pad, x2_pad, labels\n",
    "    \n",
    "        \n",
    "def load_word2vec(embedding_path, word_index, embed_dim=300): \n",
    "    \"\"\"\n",
    "    载入预训练好的word2vec(Google-News)\n",
    "    \"\"\"\n",
    "    # 载入word2vec词向量\n",
    "    word2vec_dict = KeyedVectors.load_word2vec_format(embedding_path,binary=True)\n",
    "    \n",
    "    embedding_index = dict()\n",
    "    for word in word2vec_dict.wv.vocab:\n",
    "        embedding_index[word] = word2vec_dict.word_vec(word)\n",
    "    print('Load %s word vectors.' % len(embedding_index))\n",
    "    \n",
    "    all_embs = np.stack(list(embedding_index.values()))\n",
    "    # emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    emb_mean = np.mean(all_embs, axis=0)\n",
    "    vocab_size = len(word_index)\n",
    "    # 初始化权重\n",
    "    embedding_matrix = np.zeros((vocab_size+1, embed_dim))\n",
    "    gc.collect()\n",
    "    # 对权重矩阵进行赋值，未找到时用词向量平均值填充\n",
    "    for word, i in word_index.items():\n",
    "        if i < vocab_size + 1:\n",
    "            embedding_vector = embedding_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "            else:\n",
    "                embedding_matrix[i] = emb_mean\n",
    "    del embedding_index\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 3000000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "dico = load_dict(MAPS_FILE_PATH)\n",
    "word_index, index_word = create_maps(dico)\n",
    "\n",
    "# 准备数据\n",
    "x1_train, x2_train, y_train = load_data(DATA_TRAIN_PATH, word_index)\n",
    "x1_valid, x2_valid, y_valid = load_data(DATA_VALID_PATH, word_index)\n",
    "x1_test, x2_test, y_test = load_data(DATA_TEST_PATH, word_index)\n",
    "\n",
    "embedding_matrix = load_word2vec(WORD2VEC_PATH, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(dico)\n",
    "\n",
    "class Settings(object):\n",
    "    text1_maxlen = 20\n",
    "    text2_maxlen = 20\n",
    "    embedding_dim = 300\n",
    "    vocab_size = vocab_size\n",
    "    \n",
    "    topk = 1\n",
    "    num_layers = 3\n",
    "    hidden_sizes = [100, 80, 1]\n",
    "    \n",
    "    dropout_rate = 0.2\n",
    "    \n",
    "settings = Settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "query (InputLayer)              (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "doc (InputLayer)                (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 20, 300)      25293600    query[0][0]                      \n",
      "                                                                 doc[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 20, 20)       0           embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 20, 1)        0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 20, 100)      200         lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 20, 80)       8080        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 20, 1)        81          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 20, 1)        301         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 1)        0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 20)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 20)           0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 20)           0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 1)            0           reshape_2[0][0]                  \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 2)            4           dot_2[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 25,302,266\n",
      "Trainable params: 8,666\n",
      "Non-trainable params: 25,293,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 定义网络\n",
    "initializer_gate = keras.initializers.RandomUniform(minval=-0.01, maxval=0.01, seed=11)\n",
    "initializer_fc = keras.initializers.RandomUniform(minval=-0.1, maxval=0.1, seed=11)\n",
    "\n",
    "query = Input(name='query', shape=(settings.text1_maxlen,))\n",
    "doc = Input(name='doc', shape=(settings.text2_maxlen, ))\n",
    "\n",
    "\n",
    "embedding = Embedding(settings.vocab_size + 1,\n",
    "                      settings.embedding_dim,\n",
    "                      weights=[embedding_matrix],\n",
    "                      input_length=settings.text1_maxlen,\n",
    "                      trainable=False)\n",
    "\n",
    "q_embed = embedding(query)\n",
    "d_embed = embedding(doc)\n",
    "mm = Dot(axes=[2, 2], normalize=True)([q_embed, d_embed])\n",
    "\n",
    "# compute term gating        \n",
    "w_g = Dense(1)(q_embed)\n",
    "g = Lambda(lambda x: softmax(x, axis=1), output_shape=(settings.text1_maxlen, ))(w_g)\n",
    "g = Reshape((settings.text1_maxlen,))(g)\n",
    "mm_k = Lambda(lambda x: K.tf.nn.top_k(x, k=settings.topk, sorted=True)[0])(mm)\n",
    "\n",
    "for i in range(settings.num_layers):\n",
    "    mm_k = Dense(settings.hidden_sizes[i], activation='softplus', kernel_initializer='he_uniform', bias_initializer='zeros')(mm_k)\n",
    "\n",
    "mm_k_dropout = Dropout(rate=settings.dropout_rate)(mm_k)\n",
    "mm_reshape = Reshape((settings.text1_maxlen,))(mm_k_dropout)\n",
    "mean = Dot(axes=[1, 1])([mm_reshape, g])\n",
    "\n",
    "out_ = Dense(2, activation='softmax')(mean)\n",
    "\n",
    "model = Model(inputs=[query, doc], outputs=out_)\n",
    "model.summary()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 323426 samples, validate on 40428 samples\n",
      "Epoch 1/100\n",
      "323426/323426 [==============================] - 26s 82us/step - loss: 0.5658 - acc: 0.6797 - val_loss: 0.5517 - val_acc: 0.6916\n",
      "Epoch 2/100\n",
      "323426/323426 [==============================] - 26s 81us/step - loss: 0.5658 - acc: 0.6802 - val_loss: 0.5518 - val_acc: 0.6897\n",
      "Epoch 3/100\n",
      "323426/323426 [==============================] - 26s 79us/step - loss: 0.5656 - acc: 0.6808 - val_loss: 0.5516 - val_acc: 0.6933\n",
      "Epoch 4/100\n",
      "323426/323426 [==============================] - 26s 80us/step - loss: 0.5659 - acc: 0.6809 - val_loss: 0.5517 - val_acc: 0.6918\n",
      "Epoch 5/100\n",
      "323426/323426 [==============================] - 25s 78us/step - loss: 0.5654 - acc: 0.6807 - val_loss: 0.5509 - val_acc: 0.6908\n",
      "Epoch 6/100\n",
      "323426/323426 [==============================] - 26s 80us/step - loss: 0.5652 - acc: 0.6813 - val_loss: 0.5507 - val_acc: 0.6916\n",
      "Epoch 7/100\n",
      "323426/323426 [==============================] - 26s 80us/step - loss: 0.5650 - acc: 0.6817 - val_loss: 0.5511 - val_acc: 0.6934\n",
      "Epoch 8/100\n",
      "323426/323426 [==============================] - 26s 79us/step - loss: 0.5649 - acc: 0.6815 - val_loss: 0.5512 - val_acc: 0.6884\n",
      "Epoch 9/100\n",
      "323426/323426 [==============================] - 26s 80us/step - loss: 0.5662 - acc: 0.6800 - val_loss: 0.5517 - val_acc: 0.6905\n",
      "Epoch 10/100\n",
      "323426/323426 [==============================] - 26s 81us/step - loss: 0.5654 - acc: 0.6803 - val_loss: 0.5511 - val_acc: 0.6911\n",
      "Epoch 11/100\n",
      "323426/323426 [==============================] - 26s 80us/step - loss: 0.5652 - acc: 0.6807 - val_loss: 0.5536 - val_acc: 0.6821\n",
      "Epoch 12/100\n",
      "323426/323426 [==============================] - 26s 80us/step - loss: 0.5654 - acc: 0.6804 - val_loss: 0.5512 - val_acc: 0.6881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7dae923470>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime,time\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "callback_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='acc',\n",
    "        patience=5\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=MODEL_PATH,\n",
    "        monitor='val_acc',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "adadelta=Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "# model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['binary_accuracy'])\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "\n",
    "model.fit([x1_train, x2_train], y_train, epochs=100, batch_size=128, callbacks=callback_list, validation_data=([x1_valid, x2_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vec_pred = model.predict([x1_test, x2_test], batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def to_one_hot(prob):\n",
    "    prob_shape = prob.shape\n",
    "    y_pred = np.zeros(prob_shape)\n",
    "    \n",
    "    i = 0\n",
    "    for item in prob:\n",
    "        if item[0] < item[1]:\n",
    "            y_pred[i] = np.array([0, 1])\n",
    "        else:\n",
    "            y_pred[i] = np.array([1, 0])\n",
    "        i += 1\n",
    "    return y_pred\n",
    "\n",
    "y_pred = to_one_hot(y_vec_pred)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:66.44%\tRecall:64.6%\tF-score:65.02%\n"
     ]
    }
   ],
   "source": [
    "report = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "    \n",
    "def print_prf(result):\n",
    "    precision = str(round(result[0] * 100, 2)) + \"%\"\n",
    "    recall = str(round(result[1] * 100, 2)) + \"%\"\n",
    "    f1 = str(round(result[2] * 100, 2)) + \"%\"\n",
    "    \n",
    "    strResult=\"Precision:\"+precision+\"\\tRecall:\"+recall+\"\\tF-score:\"+f1\n",
    "    print(strResult)\n",
    "    \n",
    "print_prf(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40429/40429 [==============================] - 3s 79us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5505922848465222, 0.6896039971337211]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([x1_test, x2_test], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
