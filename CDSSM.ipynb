{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "from collections import Counter\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "import gc\n",
    "    \n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "seed(1000)\n",
    "from tensorflow.random import set_random_seed\n",
    "set_random_seed(1000)\n",
    "\n",
    "import pandas\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "# from keras.layers import Layer\n",
    "from keras.engine import Layer\n",
    "from keras.engine import InputSpec\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Input, Embedding, Conv1D, Concatenate, Flatten, Dropout, Dense, Bidirectional\n",
    "from keras.layers import MaxPooling1D, Conv2D, Flatten, Reshape, MaxPooling2D, LSTM, Lambda, Highway, Dot\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns       \n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import  norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline \n",
    "\n",
    "#进行配置，使用30%的GPU\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "# 设置session\n",
    "KTF.set_session(session )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文件路径\n",
    "ORIGINAL_FILE_PATH = \"../data/original_data/quora_duplicate_questions.tsv\"\n",
    "\n",
    "DATA_ALL_PATH = \"../data/all_data.txt\"\n",
    "\n",
    "DATA_TRAIN_PATH = \"../data/train.txt\"\n",
    "DATA_VALID_PATH = \"../data/valid.txt\"\n",
    "DATA_TEST_PATH = \"../data/test.txt\"\n",
    "\n",
    "MAPS_FILE_PATH = \"../data/maps.pkl\"\n",
    "\n",
    "# Glove文件\n",
    "WORD2VEC_PATH = \"../data/GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "# Model Path\n",
    "MODEL_PATH = \"../model/cdssm/cdssm-01.h5\"\n",
    "\n",
    "# 创建文件夹\n",
    "(file_path, file_name) = os.path.split(MODEL_PATH)\n",
    "if not os.path.exists(file_path):\n",
    "    os.mkdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(dict_path):\n",
    "    with open(dict_path, 'rb') as fr:\n",
    "        return pickle.load(fr)\n",
    "        \n",
    "def create_maps(dico):\n",
    "    item2id = dico\n",
    "    id2item = dict([val, key] for key, val in dico.items())\n",
    "    return item2id, id2item\n",
    "\n",
    "def read_file(fpath):\n",
    "    \"\"\"\n",
    "    读取文件，返回\n",
    "    \"\"\"\n",
    "    question1, question2, labels = [], [], []\n",
    "    with open(fpath, 'r', encoding=\"utf-8\") as fr:\n",
    "        for line in fr.readlines():\n",
    "            try:\n",
    "                q, k, l = line.strip().split('\\t')\n",
    "            except ValueError:\n",
    "                print(line)\n",
    "            question1.append(q)\n",
    "            question2.append(k)\n",
    "            labels.append(int(l))\n",
    "            \n",
    "    return question1, question2, labels\n",
    "\n",
    "def load_data(fpath, word_to_id, max_length=20):\n",
    "    \"\"\"\n",
    "    载入数据，并将其转化为id表示\n",
    "    \"\"\"\n",
    "    question1, question2, labels = read_file(fpath)\n",
    "    \n",
    "    q1_id, q2_id = [], []\n",
    "    for i in range(len(question1)):\n",
    "        q1= question1[i].split()\n",
    "        q2 = question2[i].split()\n",
    "        q1_id.append([word_to_id[x] for x in q1 if x in word_to_id])\n",
    "        q2_id.append([word_to_id[x] for x in q2 if x in word_to_id])\n",
    "\n",
    "    # 使用keras提供的pad_sequences来将文本pad为固定长度\n",
    "    x1_pad = keras.preprocessing.sequence.pad_sequences(q1_id, max_length, padding=\"post\")\n",
    "    x2_pad = keras.preprocessing.sequence.pad_sequences(q2_id, max_length, padding=\"post\")\n",
    "    labels = keras.utils.np_utils.to_categorical(labels)\n",
    "    \n",
    "    return x1_pad, x2_pad, labels\n",
    "    \n",
    "        \n",
    "def load_word2vec(embedding_path, word_index, embed_dim=300): \n",
    "    \"\"\"\n",
    "    载入预训练好的word2vec(Google-News)\n",
    "    \"\"\"\n",
    "    # 载入word2vec词向量\n",
    "    word2vec_dict = KeyedVectors.load_word2vec_format(embedding_path,binary=True)\n",
    "    \n",
    "    embedding_index = dict()\n",
    "    for word in word2vec_dict.wv.vocab:\n",
    "        embedding_index[word] = word2vec_dict.word_vec(word)\n",
    "    print('Load %s word vectors.' % len(embedding_index))\n",
    "    \n",
    "    all_embs = np.stack(list(embedding_index.values()))\n",
    "    # emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    emb_mean = np.mean(all_embs, axis=0)\n",
    "    vocab_size = len(word_index)\n",
    "    # 初始化权重\n",
    "    embedding_matrix = np.zeros((vocab_size+1, embed_dim))\n",
    "    gc.collect()\n",
    "    # 对权重矩阵进行赋值，未找到时用词向量平均值填充\n",
    "    for word, i in word_index.items():\n",
    "        if i < vocab_size + 1:\n",
    "            embedding_vector = embedding_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "            else:\n",
    "                embedding_matrix[i] = emb_mean\n",
    "    del embedding_index\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 3000000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "dico = load_dict(MAPS_FILE_PATH)\n",
    "word_index, index_word = create_maps(dico)\n",
    "\n",
    "# 准备数据\n",
    "x1_train, x2_train, y_train = load_data(DATA_TRAIN_PATH, word_index)\n",
    "x1_valid, x2_valid, y_valid = load_data(DATA_VALID_PATH, word_index)\n",
    "x1_test, x2_test, y_test = load_data(DATA_TEST_PATH, word_index)\n",
    "\n",
    "embedding_matrix = load_word2vec(WORD2VEC_PATH, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(dico)\n",
    "\n",
    "class Settings(object):\n",
    "    text1_maxlen = 20\n",
    "    text2_maxlen = 20\n",
    "    embedding_dim = 300\n",
    "    vocab_size = vocab_size\n",
    "    \n",
    "    hidden_sizes = [32]\n",
    "    kernel_count = 64\n",
    "    kernel_size = 3\n",
    "    \n",
    "    dropout_rate = 0.2\n",
    "    \n",
    "settings = Settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "query (InputLayer)              (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "doc (InputLayer)                (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 20, 300)      25293600    query[0][0]                      \n",
      "                                                                 doc[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       57664       embedding_4[0][0]                \n",
      "                                                                 embedding_4[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           conv1d_4[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 1, 64)        0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1, 64)        0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 64)           0           max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 64)           0           max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 32)           2080        reshape_7[0][0]                  \n",
      "                                                                 reshape_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 1)            0           sequential_3[1][0]               \n",
      "                                                                 sequential_3[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 2)            4           dot_2[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 25,353,348\n",
      "Trainable params: 59,748\n",
      "Non-trainable params: 25,293,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 定义网络\n",
    "def mlp_work(input_dim):\n",
    "    seq = Sequential()\n",
    "    num_hidden_layers = len(settings.hidden_sizes)\n",
    "    assert num_hidden_layers > 0\n",
    "    if num_hidden_layers == 1:\n",
    "        seq.add(Dense(settings.hidden_sizes[0], input_shape=(input_dim,)))\n",
    "    else:\n",
    "        seq.add(Dense(settings.hidden_sizes[0], activation='relu', input_shape=(input_dim,)))\n",
    "        for i in range(num_hidden_layers - 2):\n",
    "            seq.add(Dense(settings.hidden_sizes[i+1], activation='relu'))\n",
    "            seq.add(Dropout(settings.dropout_rate))\n",
    "        seq.add(Dense(settings.hidden_sizes[num_hidden_layers-1]))\n",
    "        seq.add(Dropout(settings.dropout_rate))\n",
    "    return seq\n",
    "\n",
    "query = Input(name='query', shape=(settings.text1_maxlen,))\n",
    "doc = Input(name='doc', shape=(settings.text2_maxlen,))\n",
    "\n",
    "\n",
    "embedding = Embedding(settings.vocab_size + 1,\n",
    "                      settings.embedding_dim,\n",
    "                      weights=[embedding_matrix],\n",
    "                      input_length=settings.text1_maxlen,\n",
    "                      trainable=False)\n",
    "\n",
    "q_embed = embedding(query)\n",
    "d_embed = embedding(doc)\n",
    "\n",
    "conv1d = Conv1D(settings.kernel_count, settings.kernel_size, padding='same', activation='relu')\n",
    "q_conv = conv1d(q_embed)\n",
    "q_conv = Dropout(settings.dropout_rate)(q_conv)\n",
    "d_conv = conv1d(d_embed)\n",
    "d_conv = Dropout(settings.dropout_rate)(d_conv)\n",
    "q_pool = MaxPooling1D(settings.text1_maxlen)(q_conv)\n",
    "q_pool_re = Reshape((-1,))(q_pool)\n",
    "d_pool = MaxPooling1D(settings.text2_maxlen)(d_conv)\n",
    "d_pool_re = Reshape((-1,))(d_pool)\n",
    "\n",
    "mlp = mlp_work(settings.kernel_count)\n",
    "rq = mlp(q_pool_re)\n",
    "rd = mlp(d_pool_re)\n",
    "out_ = Dot(axes=[1, 1], normalize=True)([rq, rd])\n",
    "\n",
    "out_ = Dense(2, activation='softmax')(out_)\n",
    "\n",
    "model = Model(inputs=[query, doc], outputs=out_)\n",
    "model.summary()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 323426 samples, validate on 40428 samples\n",
      "Epoch 1/100\n",
      "323426/323426 [==============================] - 22s 68us/step - loss: 0.5638 - acc: 0.7222 - val_loss: 0.5021 - val_acc: 0.7633\n",
      "Epoch 2/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.4686 - acc: 0.7802 - val_loss: 0.4675 - val_acc: 0.7763\n",
      "Epoch 3/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.4362 - acc: 0.7954 - val_loss: 0.4602 - val_acc: 0.7799\n",
      "Epoch 4/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.4184 - acc: 0.8049 - val_loss: 0.4535 - val_acc: 0.7871\n",
      "Epoch 5/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.4065 - acc: 0.8108 - val_loss: 0.4508 - val_acc: 0.7871\n",
      "Epoch 6/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3990 - acc: 0.8140 - val_loss: 0.4494 - val_acc: 0.7885\n",
      "Epoch 7/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.3913 - acc: 0.8195 - val_loss: 0.4442 - val_acc: 0.7930\n",
      "Epoch 8/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.3844 - acc: 0.8230 - val_loss: 0.4443 - val_acc: 0.7928\n",
      "Epoch 9/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3797 - acc: 0.8263 - val_loss: 0.4529 - val_acc: 0.7885\n",
      "Epoch 10/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.3746 - acc: 0.8287 - val_loss: 0.4452 - val_acc: 0.7955\n",
      "Epoch 11/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3716 - acc: 0.8304 - val_loss: 0.4490 - val_acc: 0.7922\n",
      "Epoch 12/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3684 - acc: 0.8319 - val_loss: 0.4396 - val_acc: 0.8008\n",
      "Epoch 13/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3644 - acc: 0.8342 - val_loss: 0.4406 - val_acc: 0.7999\n",
      "Epoch 14/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3614 - acc: 0.8360 - val_loss: 0.4438 - val_acc: 0.7983\n",
      "Epoch 15/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3590 - acc: 0.8371 - val_loss: 0.4447 - val_acc: 0.7991\n",
      "Epoch 16/100\n",
      "323426/323426 [==============================] - 21s 66us/step - loss: 0.3557 - acc: 0.8396 - val_loss: 0.4463 - val_acc: 0.8004\n",
      "Epoch 17/100\n",
      "323426/323426 [==============================] - 22s 67us/step - loss: 0.3543 - acc: 0.8395 - val_loss: 0.4517 - val_acc: 0.7958\n",
      "Epoch 18/100\n",
      "323426/323426 [==============================] - 22s 67us/step - loss: 0.3516 - acc: 0.8417 - val_loss: 0.4456 - val_acc: 0.8006\n",
      "Epoch 19/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3501 - acc: 0.8425 - val_loss: 0.4452 - val_acc: 0.7997\n",
      "Epoch 20/100\n",
      "323426/323426 [==============================] - 20s 63us/step - loss: 0.3488 - acc: 0.8426 - val_loss: 0.4467 - val_acc: 0.8008\n",
      "Epoch 21/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.3463 - acc: 0.8447 - val_loss: 0.4467 - val_acc: 0.8016\n",
      "Epoch 22/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.3451 - acc: 0.8451 - val_loss: 0.4406 - val_acc: 0.8047\n",
      "Epoch 23/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.3436 - acc: 0.8458 - val_loss: 0.4464 - val_acc: 0.8017\n",
      "Epoch 24/100\n",
      "323426/323426 [==============================] - 21s 66us/step - loss: 0.3420 - acc: 0.8472 - val_loss: 0.4453 - val_acc: 0.8022\n",
      "Epoch 25/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3406 - acc: 0.8476 - val_loss: 0.4477 - val_acc: 0.8011\n",
      "Epoch 26/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3393 - acc: 0.8483 - val_loss: 0.4377 - val_acc: 0.8080\n",
      "Epoch 27/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.3376 - acc: 0.8494 - val_loss: 0.4422 - val_acc: 0.8070\n",
      "Epoch 28/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3366 - acc: 0.8496 - val_loss: 0.4474 - val_acc: 0.8023\n",
      "Epoch 29/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3350 - acc: 0.8511 - val_loss: 0.4486 - val_acc: 0.8023\n",
      "Epoch 30/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3347 - acc: 0.8510 - val_loss: 0.4463 - val_acc: 0.8044\n",
      "Epoch 31/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3329 - acc: 0.8519 - val_loss: 0.4512 - val_acc: 0.8022\n",
      "Epoch 32/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3328 - acc: 0.8519 - val_loss: 0.4455 - val_acc: 0.8066\n",
      "Epoch 33/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3311 - acc: 0.8526 - val_loss: 0.4496 - val_acc: 0.8032\n",
      "Epoch 34/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3313 - acc: 0.8526 - val_loss: 0.4488 - val_acc: 0.8040\n",
      "Epoch 35/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3291 - acc: 0.8539 - val_loss: 0.4460 - val_acc: 0.8066\n",
      "Epoch 36/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3294 - acc: 0.8541 - val_loss: 0.4503 - val_acc: 0.8043\n",
      "Epoch 37/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3281 - acc: 0.8544 - val_loss: 0.4460 - val_acc: 0.8052\n",
      "Epoch 38/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.3280 - acc: 0.8549 - val_loss: 0.4440 - val_acc: 0.8085\n",
      "Epoch 39/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3265 - acc: 0.8557 - val_loss: 0.4479 - val_acc: 0.8050\n",
      "Epoch 40/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3269 - acc: 0.8552 - val_loss: 0.4416 - val_acc: 0.8109\n",
      "Epoch 41/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3260 - acc: 0.8560 - val_loss: 0.4459 - val_acc: 0.8055\n",
      "Epoch 42/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.3249 - acc: 0.8569 - val_loss: 0.4470 - val_acc: 0.8067\n",
      "Epoch 43/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3239 - acc: 0.8563 - val_loss: 0.4500 - val_acc: 0.8037\n",
      "Epoch 44/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3228 - acc: 0.8576 - val_loss: 0.4503 - val_acc: 0.8051\n",
      "Epoch 45/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3233 - acc: 0.8573 - val_loss: 0.4480 - val_acc: 0.8058\n",
      "Epoch 46/100\n",
      "323426/323426 [==============================] - 21s 63us/step - loss: 0.3223 - acc: 0.8582 - val_loss: 0.4448 - val_acc: 0.8109\n",
      "Epoch 47/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3222 - acc: 0.8581 - val_loss: 0.4460 - val_acc: 0.8093\n",
      "Epoch 48/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3211 - acc: 0.8585 - val_loss: 0.4466 - val_acc: 0.8085\n",
      "Epoch 49/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3203 - acc: 0.8588 - val_loss: 0.4528 - val_acc: 0.8041\n",
      "Epoch 50/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.3196 - acc: 0.8588 - val_loss: 0.4471 - val_acc: 0.8085\n",
      "Epoch 51/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3190 - acc: 0.8593 - val_loss: 0.4479 - val_acc: 0.8085\n",
      "Epoch 52/100\n",
      "323426/323426 [==============================] - 20s 63us/step - loss: 0.3184 - acc: 0.8599 - val_loss: 0.4479 - val_acc: 0.8090\n",
      "Epoch 53/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.3189 - acc: 0.8602 - val_loss: 0.4506 - val_acc: 0.8063\n",
      "Epoch 54/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.3172 - acc: 0.8604 - val_loss: 0.4601 - val_acc: 0.8000\n",
      "Epoch 55/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.3161 - acc: 0.8620 - val_loss: 0.4459 - val_acc: 0.8111\n",
      "Epoch 56/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3163 - acc: 0.8617 - val_loss: 0.4523 - val_acc: 0.8062\n",
      "Epoch 57/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3154 - acc: 0.8616 - val_loss: 0.4539 - val_acc: 0.8053\n",
      "Epoch 58/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3174 - acc: 0.8605 - val_loss: 0.4499 - val_acc: 0.8069\n",
      "Epoch 59/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.3160 - acc: 0.8615 - val_loss: 0.4493 - val_acc: 0.8083\n",
      "Epoch 60/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3141 - acc: 0.8621 - val_loss: 0.4593 - val_acc: 0.8022\n",
      "Epoch 61/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3142 - acc: 0.8620 - val_loss: 0.4490 - val_acc: 0.8085\n",
      "Epoch 62/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3139 - acc: 0.8623 - val_loss: 0.4539 - val_acc: 0.8047\n",
      "Epoch 63/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.3136 - acc: 0.8627 - val_loss: 0.4500 - val_acc: 0.8090\n",
      "Epoch 64/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.3130 - acc: 0.8627 - val_loss: 0.4466 - val_acc: 0.8118\n",
      "Epoch 65/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3125 - acc: 0.8630 - val_loss: 0.4508 - val_acc: 0.8090\n",
      "Epoch 66/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3127 - acc: 0.8630 - val_loss: 0.4534 - val_acc: 0.8064\n",
      "Epoch 67/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3110 - acc: 0.8638 - val_loss: 0.4487 - val_acc: 0.8104\n",
      "Epoch 68/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3110 - acc: 0.8648 - val_loss: 0.4554 - val_acc: 0.8070\n",
      "Epoch 69/100\n",
      "323426/323426 [==============================] - 21s 63us/step - loss: 0.3109 - acc: 0.8636 - val_loss: 0.4522 - val_acc: 0.8091\n",
      "Epoch 70/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3110 - acc: 0.8638 - val_loss: 0.4535 - val_acc: 0.8075\n",
      "Epoch 71/100\n",
      "323426/323426 [==============================] - 21s 63us/step - loss: 0.3105 - acc: 0.8644 - val_loss: 0.4566 - val_acc: 0.8038\n",
      "Epoch 72/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.3109 - acc: 0.8642 - val_loss: 0.4466 - val_acc: 0.8114\n",
      "Epoch 73/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.3089 - acc: 0.8653 - val_loss: 0.4531 - val_acc: 0.8068\n",
      "Epoch 74/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3094 - acc: 0.8654 - val_loss: 0.4480 - val_acc: 0.8100\n",
      "Epoch 75/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3085 - acc: 0.8660 - val_loss: 0.4559 - val_acc: 0.8071\n",
      "Epoch 76/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3088 - acc: 0.8657 - val_loss: 0.4501 - val_acc: 0.8092\n",
      "Epoch 77/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3090 - acc: 0.8653 - val_loss: 0.4522 - val_acc: 0.8077\n",
      "Epoch 78/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3088 - acc: 0.8651 - val_loss: 0.4558 - val_acc: 0.8062\n",
      "Epoch 79/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.3075 - acc: 0.8665 - val_loss: 0.4596 - val_acc: 0.8053\n",
      "Epoch 80/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.3077 - acc: 0.8660 - val_loss: 0.4496 - val_acc: 0.8099\n",
      "Epoch 81/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.3066 - acc: 0.8667 - val_loss: 0.4516 - val_acc: 0.8091\n",
      "Epoch 82/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.3069 - acc: 0.8664 - val_loss: 0.4504 - val_acc: 0.8088\n",
      "Epoch 83/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.3056 - acc: 0.8670 - val_loss: 0.4518 - val_acc: 0.8112\n",
      "Epoch 84/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.3068 - acc: 0.8666 - val_loss: 0.4523 - val_acc: 0.8087\n",
      "Epoch 85/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3059 - acc: 0.8676 - val_loss: 0.4544 - val_acc: 0.8072\n",
      "Epoch 86/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3066 - acc: 0.8672 - val_loss: 0.4500 - val_acc: 0.8094\n",
      "Epoch 87/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3050 - acc: 0.8673 - val_loss: 0.4564 - val_acc: 0.8069\n",
      "Epoch 88/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3058 - acc: 0.8673 - val_loss: 0.4549 - val_acc: 0.8066\n",
      "Epoch 89/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.3054 - acc: 0.8679 - val_loss: 0.4476 - val_acc: 0.8111\n",
      "Epoch 90/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3060 - acc: 0.8665 - val_loss: 0.4506 - val_acc: 0.8106\n",
      "Epoch 91/100\n",
      "323426/323426 [==============================] - 21s 65us/step - loss: 0.3047 - acc: 0.8679 - val_loss: 0.4553 - val_acc: 0.8095\n",
      "Epoch 92/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3055 - acc: 0.8667 - val_loss: 0.4511 - val_acc: 0.8089\n",
      "Epoch 93/100\n",
      "323426/323426 [==============================] - 21s 64us/step - loss: 0.3035 - acc: 0.8684 - val_loss: 0.4532 - val_acc: 0.8080\n",
      "Epoch 94/100\n",
      "323426/323426 [==============================] - 20s 63us/step - loss: 0.3038 - acc: 0.8680 - val_loss: 0.4525 - val_acc: 0.8108\n",
      "Epoch 95/100\n",
      "323426/323426 [==============================] - 20s 63us/step - loss: 0.3037 - acc: 0.8681 - val_loss: 0.4579 - val_acc: 0.8074\n",
      "Epoch 96/100\n",
      "323426/323426 [==============================] - 20s 63us/step - loss: 0.3038 - acc: 0.8674 - val_loss: 0.4559 - val_acc: 0.8074\n",
      "Epoch 97/100\n",
      "323426/323426 [==============================] - 20s 63us/step - loss: 0.3027 - acc: 0.8686 - val_loss: 0.4541 - val_acc: 0.8108\n",
      "Epoch 98/100\n",
      "323426/323426 [==============================] - 20s 62us/step - loss: 0.3025 - acc: 0.8691 - val_loss: 0.4511 - val_acc: 0.8112\n",
      "Epoch 99/100\n",
      "323426/323426 [==============================] - 20s 63us/step - loss: 0.3027 - acc: 0.8687 - val_loss: 0.4532 - val_acc: 0.8090\n",
      "Epoch 100/100\n",
      "323426/323426 [==============================] - 20s 63us/step - loss: 0.3022 - acc: 0.8696 - val_loss: 0.4555 - val_acc: 0.8078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa9e0552b00>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime,time\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "callback_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='acc',\n",
    "        patience=5\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=MODEL_PATH,\n",
    "        monitor='val_acc',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "adadelta=Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "# model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['binary_accuracy'])\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "\n",
    "model.fit([x1_train, x2_train], y_train, epochs=100, batch_size=128, callbacks=callback_list, validation_data=([x1_valid, x2_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vec_pred = model.predict([x1_test, x2_test], batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def to_one_hot(prob):\n",
    "    prob_shape = prob.shape\n",
    "    y_pred = np.zeros(prob_shape)\n",
    "    \n",
    "    i = 0\n",
    "    for item in prob:\n",
    "        if item[0] < item[1]:\n",
    "            y_pred[i] = np.array([0, 1])\n",
    "        else:\n",
    "            y_pred[i] = np.array([1, 0])\n",
    "        i += 1\n",
    "    return y_pred\n",
    "\n",
    "y_pred = to_one_hot(y_vec_pred)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:79.87%\tRecall:81.89%\tF-score:80.01%\n"
     ]
    }
   ],
   "source": [
    "report = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "    \n",
    "def print_prf(result):\n",
    "    precision = str(round(result[0] * 100, 2)) + \"%\"\n",
    "    recall = str(round(result[1] * 100, 2)) + \"%\"\n",
    "    f1 = str(round(result[2] * 100, 2)) + \"%\"\n",
    "    \n",
    "    strResult=\"Precision:\"+precision+\"\\tRecall:\"+recall+\"\\tF-score:\"+f1\n",
    "    print(strResult)\n",
    "    \n",
    "print_prf(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40429/40429 [==============================] - 4s 91us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.46255457086511687, 0.8050904054050115]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([x1_test, x2_test], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
