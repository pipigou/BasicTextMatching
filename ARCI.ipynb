{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "from collections import Counter\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "import gc\n",
    "    \n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns       \n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import  norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline \n",
    "\n",
    "#进行配置，使用30%的GPU\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "# 设置session\n",
    "KTF.set_session(session )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文件路径\n",
    "ORIGINAL_FILE_PATH = \"../data/original_data/quora_duplicate_questions.tsv\"\n",
    "\n",
    "DATA_ALL_PATH = \"../data/all_data.txt\"\n",
    "\n",
    "DATA_TRAIN_PATH = \"../data/train.txt\"\n",
    "DATA_VALID_PATH = \"../data/valid.txt\"\n",
    "DATA_TEST_PATH = \"../data/test.txt\"\n",
    "\n",
    "MAPS_FILE_PATH = \"../data/maps.pkl\"\n",
    "\n",
    "# Glove文件\n",
    "WORD2VEC_PATH = \"../data/GoogleNews-vectors-negative300.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(dict_path):\n",
    "    with open(dict_path, 'rb') as fr:\n",
    "        return pickle.load(fr)\n",
    "        \n",
    "def create_maps(dico):\n",
    "    item2id = dico\n",
    "    id2item = dict([val, key] for key, val in dico.items())\n",
    "    return item2id, id2item\n",
    "\n",
    "def read_file(fpath):\n",
    "    \"\"\"\n",
    "    读取文件，返回\n",
    "    \"\"\"\n",
    "    question1, question2, labels = [], [], []\n",
    "    with open(fpath, 'r', encoding=\"utf-8\") as fr:\n",
    "        for line in fr.readlines():\n",
    "            try:\n",
    "                q, k, l = line.strip().split('\\t')\n",
    "            except ValueError:\n",
    "                print(line)\n",
    "            question1.append(q)\n",
    "            question2.append(k)\n",
    "            labels.append(int(l))\n",
    "            \n",
    "    return question1, question2, labels\n",
    "\n",
    "def load_data(fpath, word_to_id, max_length=20):\n",
    "    \"\"\"\n",
    "    载入数据，并将其转化为id表示\n",
    "    \"\"\"\n",
    "    question1, question2, labels = read_file(fpath)\n",
    "    \n",
    "    q1_id, q2_id = [], []\n",
    "    for i in range(len(question1)):\n",
    "        q1= question1[i].split()\n",
    "        q2 = question2[i].split()\n",
    "        q1_id.append([word_to_id[x] for x in q1 if x in word_to_id])\n",
    "        q2_id.append([word_to_id[x] for x in q2 if x in word_to_id])\n",
    "\n",
    "    # 使用keras提供的pad_sequences来将文本pad为固定长度\n",
    "    x1_pad = keras.preprocessing.sequence.pad_sequences(q1_id, max_length, padding=\"post\")\n",
    "    x2_pad = keras.preprocessing.sequence.pad_sequences(q2_id, max_length, padding=\"post\")\n",
    "    labels = keras.utils.np_utils.to_categorical(labels)\n",
    "    \n",
    "    return x1_pad, x2_pad, labels\n",
    "    \n",
    "        \n",
    "def load_word2vec(embedding_path, word_index, embed_dim=300): \n",
    "    \"\"\"\n",
    "    载入预训练好的word2vec(Google-News)\n",
    "    \"\"\"\n",
    "    # 载入word2vec词向量\n",
    "    word2vec_dict = KeyedVectors.load_word2vec_format(embedding_path,binary=True)\n",
    "    \n",
    "    embedding_index = dict()\n",
    "    for word in word2vec_dict.wv.vocab:\n",
    "        embedding_index[word] = word2vec_dict.word_vec(word)\n",
    "    print('Load %s word vectors.' % len(embedding_index))\n",
    "    \n",
    "    all_embs = np.stack(list(embedding_index.values()))\n",
    "    # emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    emb_mean = np.mean(all_embs, axis=0)\n",
    "    vocab_size = len(word_index)\n",
    "    # 初始化权重\n",
    "    embedding_matrix = np.zeros((vocab_size+1, embed_dim))\n",
    "    gc.collect()\n",
    "    # 对权重矩阵进行赋值，未找到时用词向量平均值填充\n",
    "    for word, i in word_index.items():\n",
    "        if i < vocab_size + 1:\n",
    "            embedding_vector = embedding_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "            else:\n",
    "                embedding_matrix[i] = emb_mean\n",
    "    del embedding_index\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 3000000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "dico = load_dict(MAPS_FILE_PATH)\n",
    "word_index, index_word = create_maps(dico)\n",
    "\n",
    "# 准备数据\n",
    "x1_train, x2_train, y_train = load_data(DATA_TRAIN_PATH, word_index)\n",
    "x1_valid, x2_valid, y_valid = load_data(DATA_VALID_PATH, word_index)\n",
    "x1_test, x2_test, y_test = load_data(DATA_TEST_PATH, word_index)\n",
    "\n",
    "embedding_matrix = load_word2vec(WORD2VEC_PATH, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(dico)\n",
    "\n",
    "class Settings(object):\n",
    "    text1_maxlen = 20\n",
    "    text2_maxlen = 20\n",
    "    embedding_dim = 300\n",
    "    vocab_size = vocab_size\n",
    "    kernel_count = 128\n",
    "    kernel_size = 3\n",
    "    q_pool_size = 4\n",
    "    d_pool_size = 4\n",
    "    dropout_rate = 0.2\n",
    "    \n",
    "settings = Settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "query (InputLayer)              (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "doc (InputLayer)                (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 20, 300)      25293600    query[0][0]                      \n",
      "                                                                 doc[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 128)      115328      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 128)      115328      embedding_3[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5, 128)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 5, 128)       0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 10, 128)      0           max_pooling1d_3[0][0]            \n",
      "                                                                 max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 1280)         0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1280)         0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            2562        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 25,526,818\n",
      "Trainable params: 233,218\n",
      "Non-trainable params: 25,293,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, Conv1D, MaxPool1D, Concatenate, Flatten, Dropout, Dense, MaxPooling1D\n",
    "from keras.models import Model\n",
    "\n",
    "# 定义网络\n",
    "query = Input(name='query', shape=(settings.text1_maxlen,))\n",
    "doc = Input(name='doc', shape=(settings.text2_maxlen,))\n",
    "\n",
    "\n",
    "embedding = Embedding(settings.vocab_size + 1,\n",
    "                      settings.embedding_dim,\n",
    "                      weights=[embedding_matrix],\n",
    "                      input_length=settings.text1_maxlen,\n",
    "                      trainable=False)\n",
    "\n",
    "q_embed = embedding(query)\n",
    "d_embed = embedding(doc)\n",
    "\n",
    "q_conv1 = Conv1D(settings.kernel_count, settings.kernel_size, padding='same') (q_embed)\n",
    "d_conv1 = Conv1D(settings.kernel_count, settings.kernel_size, padding='same') (d_embed)\n",
    "\n",
    "q_pool1 = MaxPooling1D(pool_size=settings.q_pool_size)(q_conv1)\n",
    "d_pool1 = MaxPooling1D(pool_size=settings.d_pool_size)(d_conv1)\n",
    "\n",
    "pool1 = Concatenate(axis=1)([q_pool1, d_pool1])\n",
    "\n",
    "pool1_flat = Flatten()(pool1)\n",
    "\n",
    "pool1_flat_drop = Dropout(rate=settings.dropout_rate)(pool1_flat)\n",
    "\n",
    "out_ = Dense(2, activation='softmax')(pool1_flat_drop)\n",
    "\n",
    "model = Model(inputs=[query, doc], outputs=out_)\n",
    "model.summary()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 323426 samples, validate on 40428 samples\n",
      "Epoch 1/100\n",
      "323426/323426 [==============================] - 74s 229us/step - loss: 0.5462 - acc: 0.7251 - val_loss: 0.5311 - val_acc: 0.7392\n",
      "Epoch 2/100\n",
      "323426/323426 [==============================] - 71s 218us/step - loss: 0.5056 - acc: 0.7535 - val_loss: 0.5078 - val_acc: 0.7496\n",
      "Epoch 3/100\n",
      "323426/323426 [==============================] - 70s 217us/step - loss: 0.4761 - acc: 0.7716 - val_loss: 0.5240 - val_acc: 0.7380\n",
      "Epoch 4/100\n",
      "323426/323426 [==============================] - 71s 218us/step - loss: 0.4531 - acc: 0.7854 - val_loss: 0.5058 - val_acc: 0.7609\n",
      "Epoch 5/100\n",
      "323426/323426 [==============================] - 70s 218us/step - loss: 0.4329 - acc: 0.7970 - val_loss: 0.5118 - val_acc: 0.7586\n",
      "Epoch 6/100\n",
      "323426/323426 [==============================] - 71s 219us/step - loss: 0.4167 - acc: 0.8057 - val_loss: 0.5226 - val_acc: 0.7493\n",
      "Epoch 7/100\n",
      "323426/323426 [==============================] - 70s 216us/step - loss: 0.4018 - acc: 0.8138 - val_loss: 0.5254 - val_acc: 0.7530\n",
      "Epoch 8/100\n",
      "323426/323426 [==============================] - 71s 218us/step - loss: 0.3898 - acc: 0.8207 - val_loss: 0.5262 - val_acc: 0.7693\n",
      "Epoch 9/100\n",
      "323426/323426 [==============================] - 71s 218us/step - loss: 0.3801 - acc: 0.8261 - val_loss: 0.5374 - val_acc: 0.7698\n",
      "Epoch 10/100\n",
      "323426/323426 [==============================] - 69s 214us/step - loss: 0.3706 - acc: 0.8306 - val_loss: 0.5402 - val_acc: 0.7702\n",
      "Epoch 11/100\n",
      "323426/323426 [==============================] - 71s 219us/step - loss: 0.3622 - acc: 0.8352 - val_loss: 0.5442 - val_acc: 0.7654\n",
      "Epoch 12/100\n",
      "323426/323426 [==============================] - 70s 218us/step - loss: 0.3542 - acc: 0.8397 - val_loss: 0.5473 - val_acc: 0.7681\n",
      "Epoch 13/100\n",
      "323426/323426 [==============================] - 72s 221us/step - loss: 0.3474 - acc: 0.8430 - val_loss: 0.5587 - val_acc: 0.7670\n",
      "Epoch 14/100\n",
      "323426/323426 [==============================] - 72s 223us/step - loss: 0.3420 - acc: 0.8466 - val_loss: 0.5608 - val_acc: 0.7652\n",
      "Epoch 15/100\n",
      "323426/323426 [==============================] - 71s 219us/step - loss: 0.3362 - acc: 0.8490 - val_loss: 0.5660 - val_acc: 0.7705\n",
      "Epoch 16/100\n",
      "323426/323426 [==============================] - 72s 221us/step - loss: 0.3309 - acc: 0.8521 - val_loss: 0.5760 - val_acc: 0.7722\n",
      "Epoch 17/100\n",
      "323426/323426 [==============================] - 70s 217us/step - loss: 0.3257 - acc: 0.8552 - val_loss: 0.5879 - val_acc: 0.7612\n",
      "Epoch 18/100\n",
      "323426/323426 [==============================] - 71s 219us/step - loss: 0.3206 - acc: 0.8574 - val_loss: 0.6030 - val_acc: 0.7713\n",
      "Epoch 19/100\n",
      "323426/323426 [==============================] - 71s 218us/step - loss: 0.3171 - acc: 0.8593 - val_loss: 0.5897 - val_acc: 0.7688\n",
      "Epoch 20/100\n",
      "323426/323426 [==============================] - 71s 218us/step - loss: 0.3130 - acc: 0.8613 - val_loss: 0.6246 - val_acc: 0.7732\n",
      "Epoch 21/100\n",
      "323426/323426 [==============================] - 71s 220us/step - loss: 0.3096 - acc: 0.8638 - val_loss: 0.6058 - val_acc: 0.7586\n",
      "Epoch 22/100\n",
      "323426/323426 [==============================] - 69s 215us/step - loss: 0.3064 - acc: 0.8653 - val_loss: 0.5998 - val_acc: 0.7692\n",
      "Epoch 23/100\n",
      "323426/323426 [==============================] - 69s 214us/step - loss: 0.3038 - acc: 0.8662 - val_loss: 0.6145 - val_acc: 0.7691\n",
      "Epoch 24/100\n",
      "323426/323426 [==============================] - 70s 215us/step - loss: 0.3002 - acc: 0.8690 - val_loss: 0.6121 - val_acc: 0.7655\n",
      "Epoch 25/100\n",
      "323426/323426 [==============================] - 70s 216us/step - loss: 0.2984 - acc: 0.8691 - val_loss: 0.6193 - val_acc: 0.7696\n",
      "Epoch 26/100\n",
      "323426/323426 [==============================] - 70s 218us/step - loss: 0.2948 - acc: 0.8709 - val_loss: 0.6336 - val_acc: 0.7692\n",
      "Epoch 27/100\n",
      "323426/323426 [==============================] - 70s 215us/step - loss: 0.2926 - acc: 0.8715 - val_loss: 0.6311 - val_acc: 0.7713\n",
      "Epoch 28/100\n",
      "323426/323426 [==============================] - 69s 214us/step - loss: 0.2910 - acc: 0.8731 - val_loss: 0.6452 - val_acc: 0.7705\n",
      "Epoch 29/100\n",
      "323426/323426 [==============================] - 69s 214us/step - loss: 0.2890 - acc: 0.8744 - val_loss: 0.6341 - val_acc: 0.7673\n",
      "Epoch 30/100\n",
      "323426/323426 [==============================] - 70s 215us/step - loss: 0.2868 - acc: 0.8754 - val_loss: 0.6543 - val_acc: 0.7549\n",
      "Epoch 31/100\n",
      "323426/323426 [==============================] - 70s 216us/step - loss: 0.2841 - acc: 0.8780 - val_loss: 0.6527 - val_acc: 0.7671\n",
      "Epoch 32/100\n",
      "323426/323426 [==============================] - 70s 216us/step - loss: 0.2816 - acc: 0.8777 - val_loss: 0.6625 - val_acc: 0.7602\n",
      "Epoch 33/100\n",
      "323426/323426 [==============================] - 70s 216us/step - loss: 0.2816 - acc: 0.8785 - val_loss: 0.6838 - val_acc: 0.7753\n",
      "Epoch 34/100\n",
      "323426/323426 [==============================] - 70s 217us/step - loss: 0.2791 - acc: 0.8796 - val_loss: 0.6572 - val_acc: 0.7713\n",
      "Epoch 35/100\n",
      "323426/323426 [==============================] - 70s 216us/step - loss: 0.2770 - acc: 0.8806 - val_loss: 0.6694 - val_acc: 0.7667\n",
      "Epoch 36/100\n",
      "323426/323426 [==============================] - 70s 217us/step - loss: 0.2763 - acc: 0.8812 - val_loss: 0.6796 - val_acc: 0.7767\n",
      "Epoch 37/100\n",
      "323426/323426 [==============================] - 70s 216us/step - loss: 0.2747 - acc: 0.8819 - val_loss: 0.6755 - val_acc: 0.7678\n",
      "Epoch 38/100\n",
      "323426/323426 [==============================] - 70s 217us/step - loss: 0.2722 - acc: 0.8831 - val_loss: 0.6726 - val_acc: 0.7675\n",
      "Epoch 39/100\n",
      "323426/323426 [==============================] - 71s 219us/step - loss: 0.2715 - acc: 0.8836 - val_loss: 0.6834 - val_acc: 0.7753\n",
      "Epoch 40/100\n",
      "323426/323426 [==============================] - 69s 215us/step - loss: 0.2702 - acc: 0.8845 - val_loss: 0.6867 - val_acc: 0.7683\n",
      "Epoch 41/100\n",
      "323426/323426 [==============================] - 70s 216us/step - loss: 0.2688 - acc: 0.8850 - val_loss: 0.6900 - val_acc: 0.7723\n",
      "Epoch 42/100\n",
      "323426/323426 [==============================] - 70s 215us/step - loss: 0.2670 - acc: 0.8858 - val_loss: 0.6957 - val_acc: 0.7666\n",
      "Epoch 43/100\n",
      "323426/323426 [==============================] - 69s 214us/step - loss: 0.2649 - acc: 0.8869 - val_loss: 0.7013 - val_acc: 0.7693\n",
      "Epoch 44/100\n",
      "323426/323426 [==============================] - 69s 214us/step - loss: 0.2655 - acc: 0.8869 - val_loss: 0.6946 - val_acc: 0.7665\n",
      "Epoch 45/100\n",
      "323426/323426 [==============================] - 70s 216us/step - loss: 0.2633 - acc: 0.8879 - val_loss: 0.7084 - val_acc: 0.7722\n",
      "Epoch 46/100\n",
      "323426/323426 [==============================] - 69s 214us/step - loss: 0.2628 - acc: 0.8877 - val_loss: 0.7010 - val_acc: 0.7711\n",
      "Epoch 47/100\n",
      "323426/323426 [==============================] - 70s 216us/step - loss: 0.2615 - acc: 0.8892 - val_loss: 0.7126 - val_acc: 0.7599\n",
      "Epoch 48/100\n",
      "323426/323426 [==============================] - 69s 214us/step - loss: 0.2592 - acc: 0.8902 - val_loss: 0.7213 - val_acc: 0.7733\n",
      "Epoch 49/100\n",
      "323426/323426 [==============================] - 70s 216us/step - loss: 0.2581 - acc: 0.8904 - val_loss: 0.7106 - val_acc: 0.7680\n",
      "Epoch 50/100\n",
      "323426/323426 [==============================] - 69s 215us/step - loss: 0.2583 - acc: 0.8898 - val_loss: 0.7291 - val_acc: 0.7550\n",
      "Epoch 51/100\n",
      "323426/323426 [==============================] - 70s 217us/step - loss: 0.2578 - acc: 0.8908 - val_loss: 0.7143 - val_acc: 0.7681\n",
      "Epoch 52/100\n",
      "323426/323426 [==============================] - 70s 217us/step - loss: 0.2558 - acc: 0.8919 - val_loss: 0.7253 - val_acc: 0.7702\n",
      "Epoch 53/100\n",
      "323426/323426 [==============================] - 70s 215us/step - loss: 0.2547 - acc: 0.8919 - val_loss: 0.7214 - val_acc: 0.7698\n",
      "Epoch 54/100\n",
      "323426/323426 [==============================] - 70s 216us/step - loss: 0.2541 - acc: 0.8930 - val_loss: 0.7361 - val_acc: 0.7688\n",
      "Epoch 55/100\n",
      "323426/323426 [==============================] - 70s 216us/step - loss: 0.2533 - acc: 0.8928 - val_loss: 0.7352 - val_acc: 0.7576\n",
      "Epoch 56/100\n",
      "323426/323426 [==============================] - 69s 215us/step - loss: 0.2523 - acc: 0.8936 - val_loss: 0.7364 - val_acc: 0.7692\n",
      "Epoch 57/100\n",
      "323426/323426 [==============================] - 69s 214us/step - loss: 0.2535 - acc: 0.8936 - val_loss: 0.7402 - val_acc: 0.7569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "323426/323426 [==============================] - 70s 217us/step - loss: 0.2505 - acc: 0.8947 - val_loss: 0.7519 - val_acc: 0.7741\n",
      "Epoch 59/100\n",
      "323426/323426 [==============================] - 70s 216us/step - loss: 0.2512 - acc: 0.8946 - val_loss: 0.7509 - val_acc: 0.7584\n",
      "Epoch 60/100\n",
      "323426/323426 [==============================] - 69s 214us/step - loss: 0.2489 - acc: 0.8958 - val_loss: 0.7607 - val_acc: 0.7756\n",
      "Epoch 61/100\n",
      "323426/323426 [==============================] - 70s 215us/step - loss: 0.2481 - acc: 0.8954 - val_loss: 0.7485 - val_acc: 0.7739\n",
      "Epoch 62/100\n",
      "323426/323426 [==============================] - 70s 216us/step - loss: 0.2490 - acc: 0.8960 - val_loss: 0.7457 - val_acc: 0.7716\n",
      "Epoch 63/100\n",
      "323426/323426 [==============================] - 70s 216us/step - loss: 0.2481 - acc: 0.8959 - val_loss: 0.7616 - val_acc: 0.7722\n",
      "Epoch 64/100\n",
      "323426/323426 [==============================] - 69s 214us/step - loss: 0.2467 - acc: 0.8967 - val_loss: 0.7777 - val_acc: 0.7555\n",
      "Epoch 65/100\n",
      "323426/323426 [==============================] - 71s 218us/step - loss: 0.2472 - acc: 0.8966 - val_loss: 0.7704 - val_acc: 0.7525\n",
      "Epoch 66/100\n",
      "323426/323426 [==============================] - 70s 218us/step - loss: 0.2454 - acc: 0.8969 - val_loss: 0.7447 - val_acc: 0.7650\n",
      "Epoch 67/100\n",
      "323426/323426 [==============================] - 70s 217us/step - loss: 0.2457 - acc: 0.8971 - val_loss: 0.7563 - val_acc: 0.7680\n",
      "Epoch 68/100\n",
      "323426/323426 [==============================] - 69s 213us/step - loss: 0.2455 - acc: 0.8979 - val_loss: 0.7564 - val_acc: 0.7657\n",
      "Epoch 69/100\n",
      "323426/323426 [==============================] - 69s 215us/step - loss: 0.2431 - acc: 0.8984 - val_loss: 0.7662 - val_acc: 0.7726\n",
      "Epoch 70/100\n",
      "323426/323426 [==============================] - 69s 213us/step - loss: 0.2439 - acc: 0.8977 - val_loss: 0.7566 - val_acc: 0.7679\n",
      "Epoch 71/100\n",
      "323426/323426 [==============================] - 70s 216us/step - loss: 0.2428 - acc: 0.8984 - val_loss: 0.7735 - val_acc: 0.7646\n",
      "Epoch 72/100\n",
      "323426/323426 [==============================] - 70s 217us/step - loss: 0.2420 - acc: 0.8998 - val_loss: 0.7651 - val_acc: 0.7644\n",
      "Epoch 73/100\n",
      "323426/323426 [==============================] - 71s 218us/step - loss: 0.2416 - acc: 0.8996 - val_loss: 0.7815 - val_acc: 0.7587\n",
      "Epoch 74/100\n",
      "323426/323426 [==============================] - 69s 214us/step - loss: 0.2412 - acc: 0.8995 - val_loss: 0.7852 - val_acc: 0.7692\n",
      "Epoch 75/100\n",
      "323426/323426 [==============================] - 71s 218us/step - loss: 0.2395 - acc: 0.9003 - val_loss: 0.7788 - val_acc: 0.7635\n",
      "Epoch 76/100\n",
      "323426/323426 [==============================] - 70s 216us/step - loss: 0.2395 - acc: 0.9001 - val_loss: 0.7809 - val_acc: 0.7648\n",
      "Epoch 77/100\n",
      "323426/323426 [==============================] - 70s 216us/step - loss: 0.2403 - acc: 0.9005 - val_loss: 0.7939 - val_acc: 0.7753\n",
      "Epoch 78/100\n",
      "323426/323426 [==============================] - 69s 215us/step - loss: 0.2383 - acc: 0.9013 - val_loss: 0.7840 - val_acc: 0.7714\n",
      "Epoch 79/100\n",
      "323426/323426 [==============================] - 69s 215us/step - loss: 0.2376 - acc: 0.9016 - val_loss: 0.7912 - val_acc: 0.7661\n",
      "Epoch 80/100\n",
      "323426/323426 [==============================] - 70s 217us/step - loss: 0.2379 - acc: 0.9015 - val_loss: 0.8004 - val_acc: 0.7748\n",
      "Epoch 81/100\n",
      "323426/323426 [==============================] - 70s 218us/step - loss: 0.2377 - acc: 0.9017 - val_loss: 0.7880 - val_acc: 0.7747\n",
      "Epoch 82/100\n",
      "323426/323426 [==============================] - 70s 216us/step - loss: 0.2368 - acc: 0.9021 - val_loss: 0.8046 - val_acc: 0.7736\n",
      "Epoch 83/100\n",
      "323426/323426 [==============================] - 70s 217us/step - loss: 0.2371 - acc: 0.9028 - val_loss: 0.7894 - val_acc: 0.7688\n",
      "Epoch 84/100\n",
      "323426/323426 [==============================] - 70s 217us/step - loss: 0.2370 - acc: 0.9022 - val_loss: 0.8199 - val_acc: 0.7555\n",
      "Epoch 85/100\n",
      "323426/323426 [==============================] - 70s 217us/step - loss: 0.2353 - acc: 0.9029 - val_loss: 0.8139 - val_acc: 0.7570\n",
      "Epoch 86/100\n",
      "323426/323426 [==============================] - 70s 216us/step - loss: 0.2337 - acc: 0.9038 - val_loss: 0.7993 - val_acc: 0.7714\n",
      "Epoch 87/100\n",
      "323426/323426 [==============================] - 69s 214us/step - loss: 0.2350 - acc: 0.9033 - val_loss: 0.8172 - val_acc: 0.7526\n",
      "Epoch 88/100\n",
      "323426/323426 [==============================] - 70s 218us/step - loss: 0.2344 - acc: 0.9038 - val_loss: 0.8137 - val_acc: 0.7719\n",
      "Epoch 89/100\n",
      "323426/323426 [==============================] - 70s 217us/step - loss: 0.2346 - acc: 0.9038 - val_loss: 0.8040 - val_acc: 0.7750\n",
      "Epoch 90/100\n",
      "323426/323426 [==============================] - 70s 216us/step - loss: 0.2333 - acc: 0.9040 - val_loss: 0.7990 - val_acc: 0.7645\n",
      "Epoch 91/100\n",
      "323426/323426 [==============================] - 69s 215us/step - loss: 0.2311 - acc: 0.9052 - val_loss: 0.8113 - val_acc: 0.7655\n",
      "Epoch 92/100\n",
      "323426/323426 [==============================] - 70s 216us/step - loss: 0.2338 - acc: 0.9042 - val_loss: 0.8063 - val_acc: 0.7717\n",
      "Epoch 93/100\n",
      "323426/323426 [==============================] - 71s 218us/step - loss: 0.2308 - acc: 0.9051 - val_loss: 0.8134 - val_acc: 0.7715\n",
      "Epoch 94/100\n",
      "323426/323426 [==============================] - 70s 216us/step - loss: 0.2329 - acc: 0.9043 - val_loss: 0.8104 - val_acc: 0.7766\n",
      "Epoch 95/100\n",
      "323426/323426 [==============================] - 69s 214us/step - loss: 0.2319 - acc: 0.9048 - val_loss: 0.8130 - val_acc: 0.7741\n",
      "Epoch 96/100\n",
      "323426/323426 [==============================] - 71s 218us/step - loss: 0.2319 - acc: 0.9048 - val_loss: 0.8236 - val_acc: 0.7763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4d944f3a20>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime,time\n",
    "from keras.optimizers import Adam\\\n",
    "\n",
    "callback_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='acc',\n",
    "        patience=5\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"../model/arci/arci-01.h5\",\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "adadelta=Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "# model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['binary_accuracy'])\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "\n",
    "model.fit([x1_train, x2_train], y_train, epochs=100, callbacks=callback_list, validation_data=([x1_valid, x2_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vec_pred = model.predict([x1_test, x2_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "y_pred_shape = y_vec_pred.shape\n",
    "y_pred = np.zeros(y_pred_shape)\n",
    "\n",
    "i = 0\n",
    "for item in y_vec_pred:\n",
    "    if item[0] < item[1]:\n",
    "        y_pred[i] = np.array([0, 1])\n",
    "    else:\n",
    "        y_pred[i] = np.array([1, 0])\n",
    "    i += 1\n",
    "    \n",
    "report = precision_recall_fscore_support(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7587268891912451, 0.7448385780363961, 0.7501467419196652, None)\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40429/40429 [==============================] - 3s 85us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8268117943997522, 0.7725642484384776]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([x1_test, x2_test], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
