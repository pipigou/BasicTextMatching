{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "from collections import Counter\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "import gc\n",
    "    \n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "# from keras.layers import Layer\n",
    "from keras.engine import Layer\n",
    "from keras.engine import InputSpec\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns       \n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import  norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline \n",
    "\n",
    "#进行配置，使用30%的GPU\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "# 设置session\n",
    "KTF.set_session(session )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文件路径\n",
    "ORIGINAL_FILE_PATH = \"../data/original_data/quora_duplicate_questions.tsv\"\n",
    "\n",
    "DATA_ALL_PATH = \"../data/all_data.txt\"\n",
    "\n",
    "DATA_TRAIN_PATH = \"../data/train.txt\"\n",
    "DATA_VALID_PATH = \"../data/valid.txt\"\n",
    "DATA_TEST_PATH = \"../data/test.txt\"\n",
    "\n",
    "MAPS_FILE_PATH = \"../data/maps.pkl\"\n",
    "\n",
    "# Glove文件\n",
    "WORD2VEC_PATH = \"../data/GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "# Model Path\n",
    "MODEL_PATH = \"../model/arcii/arcii-01.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(dict_path):\n",
    "    with open(dict_path, 'rb') as fr:\n",
    "        return pickle.load(fr)\n",
    "        \n",
    "def create_maps(dico):\n",
    "    item2id = dico\n",
    "    id2item = dict([val, key] for key, val in dico.items())\n",
    "    return item2id, id2item\n",
    "\n",
    "def read_file(fpath):\n",
    "    \"\"\"\n",
    "    读取文件，返回\n",
    "    \"\"\"\n",
    "    question1, question2, labels = [], [], []\n",
    "    with open(fpath, 'r', encoding=\"utf-8\") as fr:\n",
    "        for line in fr.readlines():\n",
    "            try:\n",
    "                q, k, l = line.strip().split('\\t')\n",
    "            except ValueError:\n",
    "                print(line)\n",
    "            question1.append(q)\n",
    "            question2.append(k)\n",
    "            labels.append(int(l))\n",
    "            \n",
    "    return question1, question2, labels\n",
    "\n",
    "def load_data(fpath, word_to_id, max_length=20):\n",
    "    \"\"\"\n",
    "    载入数据，并将其转化为id表示\n",
    "    \"\"\"\n",
    "    question1, question2, labels = read_file(fpath)\n",
    "    \n",
    "    q1_id, q2_id = [], []\n",
    "    for i in range(len(question1)):\n",
    "        q1= question1[i].split()\n",
    "        q2 = question2[i].split()\n",
    "        q1_id.append([word_to_id[x] for x in q1 if x in word_to_id])\n",
    "        q2_id.append([word_to_id[x] for x in q2 if x in word_to_id])\n",
    "\n",
    "    # 使用keras提供的pad_sequences来将文本pad为固定长度\n",
    "    x1_pad = keras.preprocessing.sequence.pad_sequences(q1_id, max_length, padding=\"post\")\n",
    "    x2_pad = keras.preprocessing.sequence.pad_sequences(q2_id, max_length, padding=\"post\")\n",
    "    labels = keras.utils.np_utils.to_categorical(labels)\n",
    "    \n",
    "    return x1_pad, x2_pad, labels\n",
    "    \n",
    "        \n",
    "def load_word2vec(embedding_path, word_index, embed_dim=300): \n",
    "    \"\"\"\n",
    "    载入预训练好的word2vec(Google-News)\n",
    "    \"\"\"\n",
    "    # 载入word2vec词向量\n",
    "    word2vec_dict = KeyedVectors.load_word2vec_format(embedding_path,binary=True)\n",
    "    \n",
    "    embedding_index = dict()\n",
    "    for word in word2vec_dict.wv.vocab:\n",
    "        embedding_index[word] = word2vec_dict.word_vec(word)\n",
    "    print('Load %s word vectors.' % len(embedding_index))\n",
    "    \n",
    "    all_embs = np.stack(list(embedding_index.values()))\n",
    "    # emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    emb_mean = np.mean(all_embs, axis=0)\n",
    "    vocab_size = len(word_index)\n",
    "    # 初始化权重\n",
    "    embedding_matrix = np.zeros((vocab_size+1, embed_dim))\n",
    "    gc.collect()\n",
    "    # 对权重矩阵进行赋值，未找到时用词向量平均值填充\n",
    "    for word, i in word_index.items():\n",
    "        if i < vocab_size + 1:\n",
    "            embedding_vector = embedding_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "            else:\n",
    "                embedding_matrix[i] = emb_mean\n",
    "    del embedding_index\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 3000000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "dico = load_dict(MAPS_FILE_PATH)\n",
    "word_index, index_word = create_maps(dico)\n",
    "\n",
    "# 准备数据\n",
    "x1_train, x2_train, y_train = load_data(DATA_TRAIN_PATH, word_index)\n",
    "x1_valid, x2_valid, y_valid = load_data(DATA_VALID_PATH, word_index)\n",
    "x1_test, x2_test, y_test = load_data(DATA_TEST_PATH, word_index)\n",
    "\n",
    "embedding_matrix = load_word2vec(WORD2VEC_PATH, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Match(Layer):\n",
    "    \"\"\"Layer that computes a matching matrix between samples in two tensors.\n",
    "    # Arguments\n",
    "        normalize: Whether to L2-normalize samples along the\n",
    "            dot product axis before taking the dot product.\n",
    "            If set to True, then the output of the dot product\n",
    "            is the cosine proximity between the two samples.\n",
    "        **kwargs: Standard layer keyword arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, normalize=False, match_type='dot', **kwargs):\n",
    "        super(Match, self).__init__(**kwargs)\n",
    "        self.normalize = normalize\n",
    "        self.match_type = match_type\n",
    "        self.supports_masking = True\n",
    "        if match_type not in ['dot', 'mul', 'plus', 'minus', 'concat']:\n",
    "            raise ValueError('In `Match` layer, '\n",
    "                             'param match_type=%s is unknown.' % match_type)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Used purely for shape validation.\n",
    "        if not isinstance(input_shape, list) or len(input_shape) != 2:\n",
    "            raise ValueError('A `Match` layer should be called '\n",
    "                             'on a list of 2 inputs.')\n",
    "        self.shape1 = input_shape[0]\n",
    "        self.shape2 = input_shape[1]\n",
    "        if self.shape1[0] != self.shape2[0]:\n",
    "            raise ValueError(\n",
    "                'Dimension incompatibility '\n",
    "                '%s != %s. ' % (self.shape1[0], self.shape2[0]) +\n",
    "                'Layer shapes: %s, %s' % (self.shape1, self.shape2))\n",
    "        if self.shape1[2] != self.shape2[2]:\n",
    "            raise ValueError(\n",
    "                'Dimension incompatibility '\n",
    "                '%s != %s. ' % (self.shape1[2], self.shape2[2]) +\n",
    "                'Layer shapes: %s, %s' % (self.shape1, self.shape2))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x1 = inputs[0]\n",
    "        x2 = inputs[1]\n",
    "        if self.match_type in ['dot']:\n",
    "            if self.normalize:\n",
    "                x1 = K.l2_normalize(x1, axis=2)\n",
    "                x2 = K.l2_normalize(x2, axis=2)\n",
    "            output = K.tf.einsum('abd,acd->abc', x1, x2)\n",
    "            output = K.tf.expand_dims(output, 3)\n",
    "        elif self.match_type in ['mul', 'plus', 'minus']:\n",
    "            x1_exp = K.tf.stack([x1] * self.shape2[1], 2)\n",
    "            x2_exp = K.tf.stack([x2] * self.shape1[1], 1)\n",
    "            if self.match_type == 'mul':\n",
    "                output = x1_exp * x2_exp\n",
    "            elif self.match_type == 'plus':\n",
    "                output = x1_exp + x2_exp\n",
    "            elif self.match_type == 'minus':\n",
    "                output = x1_exp - x2_exp\n",
    "        elif self.match_type in ['concat']:\n",
    "            x1_exp = K.tf.stack([x1] * self.shape2[1], axis=2)\n",
    "            x2_exp = K.tf.stack([x2] * self.shape1[1], axis=1)\n",
    "            output = K.tf.concat([x1_exp, x2_exp], axis=3)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if not isinstance(input_shape, list) or len(input_shape) != 2:\n",
    "            raise ValueError('A `Match` layer should be called '\n",
    "                             'on a list of 2 inputs.')\n",
    "        shape1 = list(input_shape[0])\n",
    "        shape2 = list(input_shape[1])\n",
    "        if len(shape1) != 3 or len(shape2) != 3:\n",
    "            raise ValueError('A `Match` layer should be called '\n",
    "                             'on 2 inputs with 3 dimensions.')\n",
    "        if shape1[0] != shape2[0] or shape1[2] != shape2[2]:\n",
    "            raise ValueError('A `Match` layer should be called '\n",
    "                             'on 2 inputs with same 0,2 dimensions.')\n",
    "\n",
    "        if self.match_type in ['dot']:\n",
    "            output_shape = [shape1[0], shape1[1], shape2[1], 1]\n",
    "        elif self.match_type in ['mul', 'plus', 'minus']:\n",
    "            output_shape = [shape1[0], shape1[1], shape2[1], shape1[2]]\n",
    "        elif self.match_type in ['concat']:\n",
    "            output_shape = [shape1[0], shape1[1], shape2[1], shape1[2]+shape2[2]]\n",
    "\n",
    "        return tuple(output_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return None\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'normalize': self.normalize,\n",
    "            'match_type': self.match_type,\n",
    "        }\n",
    "        base_config = super(Match, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "def match(inputs, axes, normalize=False, match_type='dot', **kwargs):\n",
    "    \"\"\"Functional interface to the `Match` layer.\n",
    "    # Arguments\n",
    "        inputs: A list of input tensors (with exact 2 tensors).\n",
    "        normalize: Whether to L2-normalize samples along the\n",
    "            dot product axis before taking the dot product.\n",
    "            If set to True, then the output of the dot product\n",
    "            is the cosine proximity between the two samples.\n",
    "        **kwargs: Standard layer keyword arguments.\n",
    "    # Returns\n",
    "        A tensor, the dot product matching matrix of the samples \n",
    "        from the inputs.\n",
    "    \"\"\"\n",
    "    return Match(normalize=normalize, match_type=match_type, **kwargs)(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(dico)\n",
    "\n",
    "class Settings(object):\n",
    "    text1_maxlen = 20\n",
    "    text2_maxlen = 20\n",
    "    embedding_dim = 300\n",
    "    vocab_size = vocab_size\n",
    "    \n",
    "    # match前conv配置\n",
    "    kernal_count_1d = 128\n",
    "    kernal_size_1d = 15\n",
    "    # match后conv配置\n",
    "    num_blocks = 2\n",
    "    kernal_count_2d = [64, 64]\n",
    "    kernal_size_2d = [[3, 3], [3, 3]]\n",
    "    pool_size_2d = [[2, 2], [2, 2]]\n",
    "    \n",
    "    \n",
    "    dropout_rate = 0.2\n",
    "    \n",
    "settings = Settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "query (InputLayer)              (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "doc (InputLayer)                (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 20, 300)      25293600    query[0][0]                      \n",
      "                                                                 doc[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 128)      576128      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 128)      576128      embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "match_1 (Match)                 (None, 20, 20, 128)  0           conv1d_1[0][0]                   \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 20, 20, 128)  0           match_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 20, 20, 64)   73792       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 10, 10, 64)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 10, 10, 64)   36928       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 5, 5, 64)     0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1600)         0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1600)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            3202        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 26,559,778\n",
      "Trainable params: 1,266,178\n",
      "Non-trainable params: 25,293,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, Conv1D, Concatenate, Flatten, Dropout, Dense, MaxPooling1D, Conv2D, Flatten, Reshape, MaxPooling2D\n",
    "from keras.models import Model\n",
    "\n",
    "# 定义网络\n",
    "query = Input(name='query', shape=(settings.text1_maxlen,))\n",
    "doc = Input(name='doc', shape=(settings.text2_maxlen,))\n",
    "\n",
    "\n",
    "embedding = Embedding(settings.vocab_size + 1,\n",
    "                      settings.embedding_dim,\n",
    "                      weights=[embedding_matrix],\n",
    "                      input_length=settings.text1_maxlen,\n",
    "                      trainable=False)\n",
    "\n",
    "q_embed = embedding(query)\n",
    "d_embed = embedding(doc)\n",
    "\n",
    "q_conv1 = Conv1D(settings.kernal_count_1d, settings.kernal_size_1d, padding='same') (q_embed)\n",
    "d_conv1 = Conv1D(settings.kernal_count_1d, settings.kernal_size_1d, padding='same') (d_embed)\n",
    "\n",
    "cross = Match(match_type='plus')([q_conv1, d_conv1])\n",
    "\n",
    "z = Reshape((settings.text1_maxlen, settings.text2_maxlen, -1))(cross)\n",
    "\n",
    "for i in range(settings.num_blocks):\n",
    "    z = Conv2D(filters=settings.kernal_count_2d[i], kernel_size=settings.kernal_size_2d[i], padding='same', activation='relu')(z)\n",
    "    z = MaxPooling2D(pool_size=(settings.pool_size_2d[i][0], settings.pool_size_2d[i][1]))(z)\n",
    "\n",
    "pool1_flat = Flatten()(z)\n",
    "pool1_flat_drop = Dropout(rate=settings.dropout_rate)(pool1_flat)   \n",
    "\n",
    "out_ = Dense(2, activation='softmax')(pool1_flat_drop)\n",
    "\n",
    "model = Model(inputs=[query, doc], outputs=out_)\n",
    "model.summary()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 323426 samples, validate on 40428 samples\n",
      "Epoch 1/100\n",
      "323426/323426 [==============================] - 32s 98us/step - loss: 0.5326 - acc: 0.7286 - val_loss: 0.4955 - val_acc: 0.7546\n",
      "Epoch 2/100\n",
      "323426/323426 [==============================] - 30s 93us/step - loss: 0.4739 - acc: 0.7668 - val_loss: 0.4640 - val_acc: 0.7759\n",
      "Epoch 3/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.4386 - acc: 0.7884 - val_loss: 0.4678 - val_acc: 0.7727\n",
      "Epoch 4/100\n",
      "323426/323426 [==============================] - 29s 91us/step - loss: 0.4043 - acc: 0.8092 - val_loss: 0.4510 - val_acc: 0.7851\n",
      "Epoch 5/100\n",
      "323426/323426 [==============================] - 29s 91us/step - loss: 0.3719 - acc: 0.8272 - val_loss: 0.4470 - val_acc: 0.7885\n",
      "Epoch 6/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.3418 - acc: 0.8428 - val_loss: 0.4601 - val_acc: 0.7892\n",
      "Epoch 7/100\n",
      "323426/323426 [==============================] - 30s 91us/step - loss: 0.3119 - acc: 0.8582 - val_loss: 0.4808 - val_acc: 0.7924\n",
      "Epoch 8/100\n",
      "323426/323426 [==============================] - 29s 91us/step - loss: 0.2846 - acc: 0.8725 - val_loss: 0.4912 - val_acc: 0.7950\n",
      "Epoch 9/100\n",
      "323426/323426 [==============================] - 29s 91us/step - loss: 0.2609 - acc: 0.8844 - val_loss: 0.5138 - val_acc: 0.7950\n",
      "Epoch 10/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.2401 - acc: 0.8948 - val_loss: 0.5431 - val_acc: 0.7970\n",
      "Epoch 11/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.2190 - acc: 0.9049 - val_loss: 0.5890 - val_acc: 0.7957\n",
      "Epoch 12/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.2010 - acc: 0.9136 - val_loss: 0.6380 - val_acc: 0.7908\n",
      "Epoch 13/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.1870 - acc: 0.9202 - val_loss: 0.6409 - val_acc: 0.7948\n",
      "Epoch 14/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.1746 - acc: 0.9261 - val_loss: 0.6641 - val_acc: 0.7946\n",
      "Epoch 15/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.1618 - acc: 0.9320 - val_loss: 0.7040 - val_acc: 0.7958\n",
      "Epoch 16/100\n",
      "323426/323426 [==============================] - 30s 91us/step - loss: 0.1530 - acc: 0.9364 - val_loss: 0.7432 - val_acc: 0.7957\n",
      "Epoch 17/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.1438 - acc: 0.9404 - val_loss: 0.8005 - val_acc: 0.7941\n",
      "Epoch 18/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.1363 - acc: 0.9441 - val_loss: 0.8020 - val_acc: 0.7972\n",
      "Epoch 19/100\n",
      "323426/323426 [==============================] - 30s 91us/step - loss: 0.1314 - acc: 0.9469 - val_loss: 0.8359 - val_acc: 0.7967\n",
      "Epoch 20/100\n",
      "323426/323426 [==============================] - 30s 91us/step - loss: 0.1237 - acc: 0.9497 - val_loss: 0.8485 - val_acc: 0.7946\n",
      "Epoch 21/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.1187 - acc: 0.9527 - val_loss: 0.9050 - val_acc: 0.7954\n",
      "Epoch 22/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.1148 - acc: 0.9541 - val_loss: 0.9141 - val_acc: 0.7883\n",
      "Epoch 23/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.1093 - acc: 0.9568 - val_loss: 0.8952 - val_acc: 0.7959\n",
      "Epoch 24/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.1058 - acc: 0.9586 - val_loss: 0.9054 - val_acc: 0.7964\n",
      "Epoch 25/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.1014 - acc: 0.9603 - val_loss: 0.9809 - val_acc: 0.7972\n",
      "Epoch 26/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0986 - acc: 0.9616 - val_loss: 0.9812 - val_acc: 0.7978\n",
      "Epoch 27/100\n",
      "323426/323426 [==============================] - 30s 91us/step - loss: 0.0951 - acc: 0.9631 - val_loss: 1.0293 - val_acc: 0.7936\n",
      "Epoch 28/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0932 - acc: 0.9639 - val_loss: 1.0760 - val_acc: 0.7970\n",
      "Epoch 29/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0907 - acc: 0.9649 - val_loss: 1.0685 - val_acc: 0.7977\n",
      "Epoch 30/100\n",
      "323426/323426 [==============================] - 29s 91us/step - loss: 0.0886 - acc: 0.9662 - val_loss: 1.0449 - val_acc: 0.7942\n",
      "Epoch 31/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0865 - acc: 0.9671 - val_loss: 1.0902 - val_acc: 0.7942\n",
      "Epoch 32/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0848 - acc: 0.9678 - val_loss: 1.0643 - val_acc: 0.7969\n",
      "Epoch 33/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0814 - acc: 0.9689 - val_loss: 1.1367 - val_acc: 0.7976\n",
      "Epoch 34/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0798 - acc: 0.9699 - val_loss: 1.1534 - val_acc: 0.7950\n",
      "Epoch 35/100\n",
      "323426/323426 [==============================] - 30s 93us/step - loss: 0.0790 - acc: 0.9703 - val_loss: 1.2244 - val_acc: 0.7857\n",
      "Epoch 36/100\n",
      "323426/323426 [==============================] - 30s 94us/step - loss: 0.0774 - acc: 0.9712 - val_loss: 1.1607 - val_acc: 0.7993\n",
      "Epoch 37/100\n",
      "323426/323426 [==============================] - 30s 94us/step - loss: 0.0763 - acc: 0.9717 - val_loss: 1.2059 - val_acc: 0.7902\n",
      "Epoch 38/100\n",
      "323426/323426 [==============================] - 30s 91us/step - loss: 0.0730 - acc: 0.9727 - val_loss: 1.1576 - val_acc: 0.7957\n",
      "Epoch 39/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0734 - acc: 0.9728 - val_loss: 1.2139 - val_acc: 0.7982\n",
      "Epoch 40/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0716 - acc: 0.9737 - val_loss: 1.2141 - val_acc: 0.7957\n",
      "Epoch 41/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0702 - acc: 0.9739 - val_loss: 1.1964 - val_acc: 0.7933\n",
      "Epoch 42/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0697 - acc: 0.9746 - val_loss: 1.2852 - val_acc: 0.7933\n",
      "Epoch 43/100\n",
      "323426/323426 [==============================] - 30s 91us/step - loss: 0.0687 - acc: 0.9752 - val_loss: 1.3069 - val_acc: 0.7973\n",
      "Epoch 44/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0675 - acc: 0.9754 - val_loss: 1.3197 - val_acc: 0.7933\n",
      "Epoch 45/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0659 - acc: 0.9761 - val_loss: 1.2750 - val_acc: 0.8012\n",
      "Epoch 46/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0650 - acc: 0.9764 - val_loss: 1.3710 - val_acc: 0.7875\n",
      "Epoch 47/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0658 - acc: 0.9763 - val_loss: 1.3017 - val_acc: 0.7962\n",
      "Epoch 48/100\n",
      "323426/323426 [==============================] - 30s 94us/step - loss: 0.0633 - acc: 0.9774 - val_loss: 1.3079 - val_acc: 0.7893\n",
      "Epoch 49/100\n",
      "323426/323426 [==============================] - 30s 93us/step - loss: 0.0626 - acc: 0.9778 - val_loss: 1.3276 - val_acc: 0.7986\n",
      "Epoch 50/100\n",
      "323426/323426 [==============================] - 30s 93us/step - loss: 0.0622 - acc: 0.9779 - val_loss: 1.3258 - val_acc: 0.7928\n",
      "Epoch 51/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0604 - acc: 0.9784 - val_loss: 1.3977 - val_acc: 0.7950\n",
      "Epoch 52/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0622 - acc: 0.9780 - val_loss: 1.4391 - val_acc: 0.7959\n",
      "Epoch 53/100\n",
      "323426/323426 [==============================] - 30s 91us/step - loss: 0.0599 - acc: 0.9786 - val_loss: 1.3487 - val_acc: 0.7979\n",
      "Epoch 54/100\n",
      "323426/323426 [==============================] - 29s 91us/step - loss: 0.0597 - acc: 0.9789 - val_loss: 1.3786 - val_acc: 0.7906\n",
      "Epoch 55/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0603 - acc: 0.9787 - val_loss: 1.3767 - val_acc: 0.7941\n",
      "Epoch 56/100\n",
      "323426/323426 [==============================] - 30s 93us/step - loss: 0.0570 - acc: 0.9798 - val_loss: 1.3963 - val_acc: 0.7964\n",
      "Epoch 57/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0580 - acc: 0.9797 - val_loss: 1.4014 - val_acc: 0.7937\n",
      "Epoch 58/100\n",
      "323426/323426 [==============================] - 30s 93us/step - loss: 0.0582 - acc: 0.9797 - val_loss: 1.3680 - val_acc: 0.7967\n",
      "Epoch 59/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0565 - acc: 0.9806 - val_loss: 1.4696 - val_acc: 0.7959\n",
      "Epoch 60/100\n",
      "323426/323426 [==============================] - 30s 91us/step - loss: 0.0571 - acc: 0.9802 - val_loss: 1.4650 - val_acc: 0.7931\n",
      "Epoch 61/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0560 - acc: 0.9806 - val_loss: 1.3473 - val_acc: 0.7956\n",
      "Epoch 62/100\n",
      "323426/323426 [==============================] - 30s 91us/step - loss: 0.0546 - acc: 0.9811 - val_loss: 1.4612 - val_acc: 0.7931\n",
      "Epoch 63/100\n",
      "323426/323426 [==============================] - 30s 91us/step - loss: 0.0568 - acc: 0.9808 - val_loss: 1.4621 - val_acc: 0.7947\n",
      "Epoch 64/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0574 - acc: 0.9804 - val_loss: 1.3705 - val_acc: 0.7990\n",
      "Epoch 65/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0531 - acc: 0.9817 - val_loss: 1.4700 - val_acc: 0.7965\n",
      "Epoch 66/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0530 - acc: 0.9819 - val_loss: 1.4927 - val_acc: 0.7937\n",
      "Epoch 67/100\n",
      "323426/323426 [==============================] - 29s 91us/step - loss: 0.0549 - acc: 0.9812 - val_loss: 1.5245 - val_acc: 0.7966\n",
      "Epoch 68/100\n",
      "323426/323426 [==============================] - 29s 91us/step - loss: 0.0546 - acc: 0.9815 - val_loss: 1.5378 - val_acc: 0.7891\n",
      "Epoch 69/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0539 - acc: 0.9820 - val_loss: 1.4650 - val_acc: 0.7990\n",
      "Epoch 70/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0515 - acc: 0.9824 - val_loss: 1.4863 - val_acc: 0.7947\n",
      "Epoch 71/100\n",
      "323426/323426 [==============================] - 31s 96us/step - loss: 0.0524 - acc: 0.9825 - val_loss: 1.4947 - val_acc: 0.7968\n",
      "Epoch 72/100\n",
      "323426/323426 [==============================] - 31s 95us/step - loss: 0.0527 - acc: 0.9822 - val_loss: 1.6079 - val_acc: 0.7924\n",
      "Epoch 73/100\n",
      "323426/323426 [==============================] - 30s 93us/step - loss: 0.0532 - acc: 0.9822 - val_loss: 1.5023 - val_acc: 0.8012\n",
      "Epoch 74/100\n",
      "323426/323426 [==============================] - 29s 91us/step - loss: 0.0492 - acc: 0.9833 - val_loss: 1.5286 - val_acc: 0.7954\n",
      "Epoch 75/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0502 - acc: 0.9831 - val_loss: 1.6248 - val_acc: 0.7917\n",
      "Epoch 76/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0522 - acc: 0.9825 - val_loss: 1.5258 - val_acc: 0.7835\n",
      "Epoch 77/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0502 - acc: 0.9834 - val_loss: 1.5839 - val_acc: 0.7987\n",
      "Epoch 78/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0510 - acc: 0.9834 - val_loss: 1.4738 - val_acc: 0.7986\n",
      "Epoch 79/100\n",
      "323426/323426 [==============================] - 30s 93us/step - loss: 0.0489 - acc: 0.9839 - val_loss: 1.6014 - val_acc: 0.7981\n",
      "Epoch 80/100\n",
      "323426/323426 [==============================] - 30s 93us/step - loss: 0.0482 - acc: 0.9839 - val_loss: 1.6083 - val_acc: 0.7980\n",
      "Epoch 81/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0494 - acc: 0.9836 - val_loss: 1.5654 - val_acc: 0.7977\n",
      "Epoch 82/100\n",
      "323426/323426 [==============================] - 29s 91us/step - loss: 0.0510 - acc: 0.9834 - val_loss: 1.5724 - val_acc: 0.7954\n",
      "Epoch 83/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0478 - acc: 0.9843 - val_loss: 1.5520 - val_acc: 0.8010\n",
      "Epoch 84/100\n",
      "323426/323426 [==============================] - 30s 91us/step - loss: 0.0497 - acc: 0.9841 - val_loss: 1.5260 - val_acc: 0.7999\n",
      "Epoch 85/100\n",
      "323426/323426 [==============================] - 30s 91us/step - loss: 0.0490 - acc: 0.9842 - val_loss: 1.7610 - val_acc: 0.7943\n",
      "Epoch 86/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0487 - acc: 0.9842 - val_loss: 1.5729 - val_acc: 0.7949\n",
      "Epoch 87/100\n",
      "323426/323426 [==============================] - 30s 91us/step - loss: 0.0489 - acc: 0.9844 - val_loss: 1.5694 - val_acc: 0.7954\n",
      "Epoch 88/100\n",
      "323426/323426 [==============================] - 30s 91us/step - loss: 0.0493 - acc: 0.9842 - val_loss: 1.6612 - val_acc: 0.7956\n",
      "Epoch 89/100\n",
      "323426/323426 [==============================] - 30s 91us/step - loss: 0.0490 - acc: 0.9841 - val_loss: 1.6221 - val_acc: 0.7954\n",
      "Epoch 90/100\n",
      "323426/323426 [==============================] - 30s 91us/step - loss: 0.0487 - acc: 0.9845 - val_loss: 1.6660 - val_acc: 0.7959\n",
      "Epoch 91/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0468 - acc: 0.9852 - val_loss: 1.7121 - val_acc: 0.7944\n",
      "Epoch 92/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0478 - acc: 0.9846 - val_loss: 1.6570 - val_acc: 0.7972\n",
      "Epoch 93/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0471 - acc: 0.9848 - val_loss: 1.5539 - val_acc: 0.7931\n",
      "Epoch 94/100\n",
      "323426/323426 [==============================] - 30s 93us/step - loss: 0.0471 - acc: 0.9849 - val_loss: 1.6337 - val_acc: 0.7968\n",
      "Epoch 95/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0477 - acc: 0.9849 - val_loss: 1.7330 - val_acc: 0.7984\n",
      "Epoch 96/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0455 - acc: 0.9858 - val_loss: 1.6689 - val_acc: 0.7961\n",
      "Epoch 97/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0470 - acc: 0.9850 - val_loss: 1.6721 - val_acc: 0.7968\n",
      "Epoch 98/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0446 - acc: 0.9855 - val_loss: 1.7627 - val_acc: 0.7911\n",
      "Epoch 99/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0465 - acc: 0.9852 - val_loss: 1.7850 - val_acc: 0.7922\n",
      "Epoch 100/100\n",
      "323426/323426 [==============================] - 30s 92us/step - loss: 0.0473 - acc: 0.9851 - val_loss: 1.6359 - val_acc: 0.7991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbbb0522e80>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime,time\n",
    "from keras.optimizers import Adam\\\n",
    "\n",
    "callback_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='acc',\n",
    "        patience=5\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=MODEL_PATH,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "adadelta=Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "# model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['binary_accuracy'])\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "\n",
    "model.fit([x1_train, x2_train], y_train, epochs=100, batch_size=128, callbacks=callback_list, validation_data=([x1_valid, x2_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vec_pred = model.predict([x1_test, x2_test], batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def to_one_hot(prob):\n",
    "    prob_shape = prob.shape\n",
    "    y_pred = np.zeros(prob_shape)\n",
    "    \n",
    "    i = 0\n",
    "    for item in prob:\n",
    "        if item[0] < item[1]:\n",
    "            y_pred[i] = np.array([0, 1])\n",
    "        else:\n",
    "            y_pred[i] = np.array([1, 0])\n",
    "        i += 1\n",
    "    return y_pred\n",
    "\n",
    "y_pred = to_one_hot(y_vec_pred)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:78.35%\tRecall:77.63%\tF-score:77.94%\n"
     ]
    }
   ],
   "source": [
    "report = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "    \n",
    "def print_prf(result):\n",
    "    precision = str(round(result[0] * 100, 2)) + \"%\"\n",
    "    recall = str(round(result[1] * 100, 2)) + \"%\"\n",
    "    f1 = str(round(result[2] * 100, 2)) + \"%\"\n",
    "    \n",
    "    strResult=\"Precision:\"+precision+\"\\tRecall:\"+recall+\"\\tF-score:\"+f1\n",
    "    print(strResult)\n",
    "    \n",
    "print_prf(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40429/40429 [==============================] - 4s 105us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6730656076814499, 0.7966311311261229]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([x1_test, x2_test], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
