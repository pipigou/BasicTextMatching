{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "from collections import Counter\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "import gc\n",
    "    \n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "seed(1000)\n",
    "from tensorflow.random import set_random_seed\n",
    "set_random_seed(1000)\n",
    "\n",
    "import pandas\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "# from keras.layers import Layer\n",
    "from keras.engine import Layer\n",
    "from keras.engine import InputSpec\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Input, Embedding, Conv1D, Concatenate, Flatten, Dropout, Dense, Bidirectional, Activation\n",
    "from keras.layers import MaxPooling1D, Conv2D, Flatten, Reshape, MaxPooling2D, LSTM, Lambda, Highway, Dot, Permute, Add\n",
    "from keras.models import Model, Sequential\n",
    "from keras.activations import softmax\n",
    "from keras.initializers import Constant, RandomNormal, RandomUniform\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns       \n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import  norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline \n",
    "\n",
    "#进行配置，使用30%的GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "# 设置session\n",
    "KTF.set_session(session )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文件路径\n",
    "ORIGINAL_FILE_PATH = \"../data/original_data/quora_duplicate_questions.tsv\"\n",
    "\n",
    "DATA_ALL_PATH = \"../data/all_data.txt\"\n",
    "\n",
    "DATA_TRAIN_PATH = \"../data/train.txt\"\n",
    "DATA_VALID_PATH = \"../data/valid.txt\"\n",
    "DATA_TEST_PATH = \"../data/test.txt\"\n",
    "\n",
    "MAPS_FILE_PATH = \"../data/maps.pkl\"\n",
    "\n",
    "# Glove文件\n",
    "WORD2VEC_PATH = \"../data/GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "# Model Path\n",
    "MODEL_PATH = \"../model/mvlstm/mvlstm-01.h5\"\n",
    "\n",
    "# 创建文件夹\n",
    "(file_path, file_name) = os.path.split(MODEL_PATH)\n",
    "if not os.path.exists(file_path):\n",
    "    os.mkdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(dict_path):\n",
    "    with open(dict_path, 'rb') as fr:\n",
    "        return pickle.load(fr)\n",
    "        \n",
    "def create_maps(dico):\n",
    "    item2id = dico\n",
    "    id2item = dict([val, key] for key, val in dico.items())\n",
    "    return item2id, id2item\n",
    "\n",
    "def read_file(fpath):\n",
    "    \"\"\"\n",
    "    读取文件，返回\n",
    "    \"\"\"\n",
    "    question1, question2, labels = [], [], []\n",
    "    with open(fpath, 'r', encoding=\"utf-8\") as fr:\n",
    "        for line in fr.readlines():\n",
    "            try:\n",
    "                q, k, l = line.strip().split('\\t')\n",
    "            except ValueError:\n",
    "                print(line)\n",
    "            question1.append(q)\n",
    "            question2.append(k)\n",
    "            labels.append(int(l))\n",
    "            \n",
    "    return question1, question2, labels\n",
    "\n",
    "def load_data(fpath, word_to_id, max_length=20):\n",
    "    \"\"\"\n",
    "    载入数据，并将其转化为id表示\n",
    "    \"\"\"\n",
    "    question1, question2, labels = read_file(fpath)\n",
    "    \n",
    "    q1_id, q2_id = [], []\n",
    "    for i in range(len(question1)):\n",
    "        q1= question1[i].split()\n",
    "        q2 = question2[i].split()\n",
    "        q1_id.append([word_to_id[x] for x in q1 if x in word_to_id])\n",
    "        q2_id.append([word_to_id[x] for x in q2 if x in word_to_id])\n",
    "\n",
    "    # 使用keras提供的pad_sequences来将文本pad为固定长度\n",
    "    x1_pad = keras.preprocessing.sequence.pad_sequences(q1_id, max_length, padding=\"post\")\n",
    "    x2_pad = keras.preprocessing.sequence.pad_sequences(q2_id, max_length, padding=\"post\")\n",
    "    labels = keras.utils.np_utils.to_categorical(labels)\n",
    "    \n",
    "    return x1_pad, x2_pad, labels\n",
    "    \n",
    "        \n",
    "def load_word2vec(embedding_path, word_index, embed_dim=300): \n",
    "    \"\"\"\n",
    "    载入预训练好的word2vec(Google-News)\n",
    "    \"\"\"\n",
    "    # 载入word2vec词向量\n",
    "    word2vec_dict = KeyedVectors.load_word2vec_format(embedding_path,binary=True)\n",
    "    \n",
    "    embedding_index = dict()\n",
    "    for word in word2vec_dict.wv.vocab:\n",
    "        embedding_index[word] = word2vec_dict.word_vec(word)\n",
    "    print('Load %s word vectors.' % len(embedding_index))\n",
    "    \n",
    "    all_embs = np.stack(list(embedding_index.values()))\n",
    "    # emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    emb_mean = np.mean(all_embs, axis=0)\n",
    "    vocab_size = len(word_index)\n",
    "    # 初始化权重\n",
    "    embedding_matrix = np.zeros((vocab_size+1, embed_dim))\n",
    "    gc.collect()\n",
    "    # 对权重矩阵进行赋值，未找到时用词向量平均值填充\n",
    "    for word, i in word_index.items():\n",
    "        if i < vocab_size + 1:\n",
    "            embedding_vector = embedding_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "            else:\n",
    "                embedding_matrix[i] = emb_mean\n",
    "    del embedding_index\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 3000000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "dico = load_dict(MAPS_FILE_PATH)\n",
    "word_index, index_word = create_maps(dico)\n",
    "\n",
    "# 准备数据\n",
    "x1_train, x2_train, y_train = load_data(DATA_TRAIN_PATH, word_index)\n",
    "x1_valid, x2_valid, y_valid = load_data(DATA_VALID_PATH, word_index)\n",
    "x1_test, x2_test, y_test = load_data(DATA_TEST_PATH, word_index)\n",
    "\n",
    "embedding_matrix = load_word2vec(WORD2VEC_PATH, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.engine import Layer\n",
    "from keras.engine import InputSpec\n",
    "\n",
    "class Match(Layer):\n",
    "    \"\"\"Layer that computes a matching matrix between samples in two tensors.\n",
    "    # Arguments\n",
    "        normalize: Whether to L2-normalize samples along the\n",
    "            dot product axis before taking the dot product.\n",
    "            If set to True, then the output of the dot product\n",
    "            is the cosine proximity between the two samples.\n",
    "        **kwargs: Standard layer keyword arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, normalize=False, match_type='dot', **kwargs):\n",
    "        super(Match, self).__init__(**kwargs)\n",
    "        self.normalize = normalize\n",
    "        self.match_type = match_type\n",
    "        self.supports_masking = True\n",
    "        if match_type not in ['dot', 'mul', 'plus', 'minus', 'concat']:\n",
    "            raise ValueError('In `Match` layer, '\n",
    "                             'param match_type=%s is unknown.' % match_type)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Used purely for shape validation.\n",
    "        if not isinstance(input_shape, list) or len(input_shape) != 2:\n",
    "            raise ValueError('A `Match` layer should be called '\n",
    "                             'on a list of 2 inputs.')\n",
    "        self.shape1 = input_shape[0]\n",
    "        self.shape2 = input_shape[1]\n",
    "        if self.shape1[0] != self.shape2[0]:\n",
    "            raise ValueError(\n",
    "                'Dimension incompatibility '\n",
    "                '%s != %s. ' % (self.shape1[0], self.shape2[0]) +\n",
    "                'Layer shapes: %s, %s' % (self.shape1, self.shape2))\n",
    "        if self.shape1[2] != self.shape2[2]:\n",
    "            raise ValueError(\n",
    "                'Dimension incompatibility '\n",
    "                '%s != %s. ' % (self.shape1[2], self.shape2[2]) +\n",
    "                'Layer shapes: %s, %s' % (self.shape1, self.shape2))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x1 = inputs[0]\n",
    "        x2 = inputs[1]\n",
    "        if self.match_type in ['dot']:\n",
    "            if self.normalize:\n",
    "                x1 = K.l2_normalize(x1, axis=2)\n",
    "                x2 = K.l2_normalize(x2, axis=2)\n",
    "            output = K.tf.einsum('abd,acd->abc', x1, x2)\n",
    "            output = K.tf.expand_dims(output, 3)\n",
    "        elif self.match_type in ['mul', 'plus', 'minus']:\n",
    "            x1_exp = K.tf.stack([x1] * self.shape2[1], 2)\n",
    "            x2_exp = K.tf.stack([x2] * self.shape1[1], 1)\n",
    "            if self.match_type == 'mul':\n",
    "                output = x1_exp * x2_exp\n",
    "            elif self.match_type == 'plus':\n",
    "                output = x1_exp + x2_exp\n",
    "            elif self.match_type == 'minus':\n",
    "                output = x1_exp - x2_exp\n",
    "        elif self.match_type in ['concat']:\n",
    "            x1_exp = K.tf.stack([x1] * self.shape2[1], axis=2)\n",
    "            x2_exp = K.tf.stack([x2] * self.shape1[1], axis=1)\n",
    "            output = K.tf.concat([x1_exp, x2_exp], axis=3)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if not isinstance(input_shape, list) or len(input_shape) != 2:\n",
    "            raise ValueError('A `Match` layer should be called '\n",
    "                             'on a list of 2 inputs.')\n",
    "        shape1 = list(input_shape[0])\n",
    "        shape2 = list(input_shape[1])\n",
    "        if len(shape1) != 3 or len(shape2) != 3:\n",
    "            raise ValueError('A `Match` layer should be called '\n",
    "                             'on 2 inputs with 3 dimensions.')\n",
    "        if shape1[0] != shape2[0] or shape1[2] != shape2[2]:\n",
    "            raise ValueError('A `Match` layer should be called '\n",
    "                             'on 2 inputs with same 0,2 dimensions.')\n",
    "\n",
    "        if self.match_type in ['dot']:\n",
    "            output_shape = [shape1[0], shape1[1], shape2[1], 1]\n",
    "        elif self.match_type in ['mul', 'plus', 'minus']:\n",
    "            output_shape = [shape1[0], shape1[1], shape2[1], shape1[2]]\n",
    "        elif self.match_type in ['concat']:\n",
    "            output_shape = [shape1[0], shape1[1], shape2[1], shape1[2]+shape2[2]]\n",
    "\n",
    "        return tuple(output_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return None\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'normalize': self.normalize,\n",
    "            'match_type': self.match_type,\n",
    "        }\n",
    "        base_config = super(Match, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "def match(inputs, axes, normalize=False, match_type='dot', **kwargs):\n",
    "    \"\"\"Functional interface to the `Match` layer.\n",
    "    # Arguments\n",
    "        inputs: A list of input tensors (with exact 2 tensors).\n",
    "        normalize: Whether to L2-normalize samples along the\n",
    "            dot product axis before taking the dot product.\n",
    "            If set to True, then the output of the dot product\n",
    "            is the cosine proximity between the two samples.\n",
    "        **kwargs: Standard layer keyword arguments.\n",
    "    # Returns\n",
    "        A tensor, the dot product matching matrix of the samples \n",
    "        from the inputs.\n",
    "    \"\"\"\n",
    "    return Match(normalize=normalize, match_type=match_type, **kwargs)(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(dico)\n",
    "\n",
    "class Settings(object):\n",
    "    text1_maxlen = 20\n",
    "    text2_maxlen = 20\n",
    "    embedding_dim = 300\n",
    "    vocab_size = vocab_size\n",
    "    \n",
    "    hidden_sizes = 50\n",
    "    drop_out = 0.2\n",
    "    topk = 30\n",
    "    \n",
    "settings = Settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "query (InputLayer)              (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "doc (InputLayer)                (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 20, 300)      25293600    query[0][0]                      \n",
      "                                                                 doc[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 20, 100)      140400      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 20, 100)      140400      embedding_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "match_1 (Match)                 (None, 20, 20, 1)    0           bidirectional_3[0][0]            \n",
      "                                                                 bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 400)          0           match_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 30)           0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 30)           0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            62          dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 25,574,462\n",
      "Trainable params: 280,862\n",
      "Non-trainable params: 25,293,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 定义网络\n",
    "query = Input(name='query', shape=(settings.text1_maxlen,))\n",
    "doc = Input(name='doc', shape=(settings.text2_maxlen,))\n",
    "\n",
    "embedding = Embedding(settings.vocab_size + 1,\n",
    "                      settings.embedding_dim,\n",
    "                      weights=[embedding_matrix],\n",
    "                      input_length=settings.text1_maxlen,\n",
    "                      trainable=False)\n",
    "\n",
    "q_embed = embedding(query)\n",
    "d_embed = embedding(doc)\n",
    "\n",
    "q_rep = Bidirectional(LSTM(settings.hidden_sizes, return_sequences=True, dropout=settings.drop_out))(q_embed)\n",
    "d_rep = Bidirectional(LSTM(settings.hidden_sizes, return_sequences=True, dropout=settings.drop_out))(d_embed)\n",
    "\n",
    "cross = Match(match_type='dot')([q_rep, d_rep])\n",
    "\n",
    "cross_reshape = Reshape((-1, ))(cross)\n",
    "\n",
    "mm_k = Lambda(lambda x: K.tf.nn.top_k(x, k=settings.topk, sorted=True)[0])(cross_reshape)\n",
    "\n",
    "pool1_flat_drop = Dropout(rate=settings.drop_out)(mm_k)\n",
    "\n",
    "out_ = Dense(2, activation='softmax')(pool1_flat_drop)\n",
    "\n",
    "model = Model(inputs=[query, doc], outputs=out_)\n",
    "model.summary()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 323426 samples, validate on 40428 samples\n",
      "Epoch 1/100\n",
      "323426/323426 [==============================] - 441s 1ms/step - loss: 0.5551 - acc: 0.7098 - val_loss: 0.5121 - val_acc: 0.7412\n",
      "Epoch 2/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.5009 - acc: 0.7468 - val_loss: 0.4831 - val_acc: 0.7572\n",
      "Epoch 3/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.4715 - acc: 0.7655 - val_loss: 0.4599 - val_acc: 0.7732\n",
      "Epoch 4/100\n",
      "323426/323426 [==============================] - 377s 1ms/step - loss: 0.4479 - acc: 0.7807 - val_loss: 0.4420 - val_acc: 0.7836\n",
      "Epoch 5/100\n",
      "323426/323426 [==============================] - 375s 1ms/step - loss: 0.4267 - acc: 0.7931 - val_loss: 0.4234 - val_acc: 0.7950\n",
      "Epoch 6/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.4096 - acc: 0.8031 - val_loss: 0.4175 - val_acc: 0.7995\n",
      "Epoch 7/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.3939 - acc: 0.8132 - val_loss: 0.4131 - val_acc: 0.8032\n",
      "Epoch 8/100\n",
      "323426/323426 [==============================] - 377s 1ms/step - loss: 0.3805 - acc: 0.8210 - val_loss: 0.4028 - val_acc: 0.8084\n",
      "Epoch 9/100\n",
      "323426/323426 [==============================] - 378s 1ms/step - loss: 0.3674 - acc: 0.8288 - val_loss: 0.3999 - val_acc: 0.8133\n",
      "Epoch 10/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.3563 - acc: 0.8350 - val_loss: 0.3975 - val_acc: 0.8140\n",
      "Epoch 11/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.3465 - acc: 0.8405 - val_loss: 0.3977 - val_acc: 0.8181\n",
      "Epoch 12/100\n",
      "323426/323426 [==============================] - 374s 1ms/step - loss: 0.3382 - acc: 0.8450 - val_loss: 0.4058 - val_acc: 0.8141\n",
      "Epoch 13/100\n",
      "323426/323426 [==============================] - 374s 1ms/step - loss: 0.3300 - acc: 0.8495 - val_loss: 0.3998 - val_acc: 0.8179\n",
      "Epoch 14/100\n",
      "323426/323426 [==============================] - 375s 1ms/step - loss: 0.3225 - acc: 0.8541 - val_loss: 0.4015 - val_acc: 0.8186\n",
      "Epoch 15/100\n",
      "323426/323426 [==============================] - 375s 1ms/step - loss: 0.3148 - acc: 0.8578 - val_loss: 0.4026 - val_acc: 0.8187\n",
      "Epoch 16/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.3077 - acc: 0.8617 - val_loss: 0.3924 - val_acc: 0.8242\n",
      "Epoch 17/100\n",
      "323426/323426 [==============================] - 375s 1ms/step - loss: 0.3031 - acc: 0.8643 - val_loss: 0.3987 - val_acc: 0.8252\n",
      "Epoch 18/100\n",
      "323426/323426 [==============================] - 375s 1ms/step - loss: 0.2982 - acc: 0.8666 - val_loss: 0.3972 - val_acc: 0.8259\n",
      "Epoch 19/100\n",
      "323426/323426 [==============================] - 375s 1ms/step - loss: 0.2921 - acc: 0.8696 - val_loss: 0.3969 - val_acc: 0.8275\n",
      "Epoch 20/100\n",
      "323426/323426 [==============================] - 374s 1ms/step - loss: 0.2878 - acc: 0.8721 - val_loss: 0.4111 - val_acc: 0.8235\n",
      "Epoch 21/100\n",
      "323426/323426 [==============================] - 379s 1ms/step - loss: 0.2828 - acc: 0.8749 - val_loss: 0.4058 - val_acc: 0.8245\n",
      "Epoch 22/100\n",
      "323426/323426 [==============================] - 384s 1ms/step - loss: 0.2799 - acc: 0.8757 - val_loss: 0.4032 - val_acc: 0.8271\n",
      "Epoch 23/100\n",
      "323426/323426 [==============================] - 382s 1ms/step - loss: 0.2757 - acc: 0.8782 - val_loss: 0.4088 - val_acc: 0.8264\n",
      "Epoch 24/100\n",
      "323426/323426 [==============================] - 382s 1ms/step - loss: 0.2718 - acc: 0.8797 - val_loss: 0.4155 - val_acc: 0.8258\n",
      "Epoch 25/100\n",
      "323426/323426 [==============================] - 383s 1ms/step - loss: 0.2686 - acc: 0.8818 - val_loss: 0.4154 - val_acc: 0.8267\n",
      "Epoch 26/100\n",
      "323426/323426 [==============================] - 385s 1ms/step - loss: 0.2658 - acc: 0.8827 - val_loss: 0.4133 - val_acc: 0.8266\n",
      "Epoch 27/100\n",
      "323426/323426 [==============================] - 383s 1ms/step - loss: 0.2622 - acc: 0.8851 - val_loss: 0.4129 - val_acc: 0.8256\n",
      "Epoch 28/100\n",
      "323426/323426 [==============================] - 383s 1ms/step - loss: 0.2592 - acc: 0.8865 - val_loss: 0.4240 - val_acc: 0.8266\n",
      "Epoch 29/100\n",
      "323426/323426 [==============================] - 384s 1ms/step - loss: 0.2579 - acc: 0.8875 - val_loss: 0.4216 - val_acc: 0.8262\n",
      "Epoch 30/100\n",
      "323426/323426 [==============================] - 379s 1ms/step - loss: 0.2547 - acc: 0.8889 - val_loss: 0.4256 - val_acc: 0.8266\n",
      "Epoch 31/100\n",
      "323426/323426 [==============================] - 375s 1ms/step - loss: 0.2518 - acc: 0.8903 - val_loss: 0.4215 - val_acc: 0.8298\n",
      "Epoch 32/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.2505 - acc: 0.8907 - val_loss: 0.4204 - val_acc: 0.8279\n",
      "Epoch 33/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.2483 - acc: 0.8915 - val_loss: 0.4244 - val_acc: 0.8300\n",
      "Epoch 34/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.2455 - acc: 0.8935 - val_loss: 0.4270 - val_acc: 0.8278\n",
      "Epoch 35/100\n",
      "323426/323426 [==============================] - 377s 1ms/step - loss: 0.2438 - acc: 0.8939 - val_loss: 0.4290 - val_acc: 0.8267\n",
      "Epoch 36/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.2416 - acc: 0.8946 - val_loss: 0.4321 - val_acc: 0.8281\n",
      "Epoch 37/100\n",
      "323426/323426 [==============================] - 375s 1ms/step - loss: 0.2400 - acc: 0.8962 - val_loss: 0.4278 - val_acc: 0.8276\n",
      "Epoch 38/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.2382 - acc: 0.8968 - val_loss: 0.4283 - val_acc: 0.8292\n",
      "Epoch 39/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.2363 - acc: 0.8983 - val_loss: 0.4346 - val_acc: 0.8272\n",
      "Epoch 40/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.2358 - acc: 0.8974 - val_loss: 0.4334 - val_acc: 0.8295\n",
      "Epoch 41/100\n",
      "323426/323426 [==============================] - 378s 1ms/step - loss: 0.2326 - acc: 0.9001 - val_loss: 0.4414 - val_acc: 0.8279\n",
      "Epoch 42/100\n",
      "323426/323426 [==============================] - 378s 1ms/step - loss: 0.2319 - acc: 0.8998 - val_loss: 0.4399 - val_acc: 0.8295\n",
      "Epoch 43/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.2305 - acc: 0.9002 - val_loss: 0.4474 - val_acc: 0.8287\n",
      "Epoch 44/100\n",
      "323426/323426 [==============================] - 377s 1ms/step - loss: 0.2290 - acc: 0.9011 - val_loss: 0.4531 - val_acc: 0.8279\n",
      "Epoch 45/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.2286 - acc: 0.9017 - val_loss: 0.4392 - val_acc: 0.8297\n",
      "Epoch 46/100\n",
      "323426/323426 [==============================] - 375s 1ms/step - loss: 0.2260 - acc: 0.9026 - val_loss: 0.4509 - val_acc: 0.8288\n",
      "Epoch 47/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.2252 - acc: 0.9032 - val_loss: 0.4540 - val_acc: 0.8263\n",
      "Epoch 48/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.2237 - acc: 0.9039 - val_loss: 0.4459 - val_acc: 0.8301\n",
      "Epoch 49/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.2233 - acc: 0.9045 - val_loss: 0.4483 - val_acc: 0.8296\n",
      "Epoch 50/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.2217 - acc: 0.9048 - val_loss: 0.4521 - val_acc: 0.8294\n",
      "Epoch 51/100\n",
      "323426/323426 [==============================] - 375s 1ms/step - loss: 0.2216 - acc: 0.9055 - val_loss: 0.4506 - val_acc: 0.8296\n",
      "Epoch 52/100\n",
      "323426/323426 [==============================] - 375s 1ms/step - loss: 0.2214 - acc: 0.9052 - val_loss: 0.4484 - val_acc: 0.8294\n",
      "Epoch 53/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.2192 - acc: 0.9059 - val_loss: 0.4580 - val_acc: 0.8303\n",
      "Epoch 54/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.2182 - acc: 0.9074 - val_loss: 0.4510 - val_acc: 0.8303\n",
      "Epoch 55/100\n",
      "323426/323426 [==============================] - 375s 1ms/step - loss: 0.2163 - acc: 0.9075 - val_loss: 0.4569 - val_acc: 0.8307\n",
      "Epoch 56/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.2178 - acc: 0.9071 - val_loss: 0.4569 - val_acc: 0.8285\n",
      "Epoch 57/100\n",
      "323426/323426 [==============================] - 374s 1ms/step - loss: 0.2159 - acc: 0.9079 - val_loss: 0.4547 - val_acc: 0.8310\n",
      "Epoch 58/100\n",
      "323426/323426 [==============================] - 375s 1ms/step - loss: 0.2153 - acc: 0.9082 - val_loss: 0.4558 - val_acc: 0.8287\n",
      "Epoch 59/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.2138 - acc: 0.9093 - val_loss: 0.4616 - val_acc: 0.8303\n",
      "Epoch 60/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.2128 - acc: 0.9096 - val_loss: 0.4635 - val_acc: 0.8296\n",
      "Epoch 61/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.2124 - acc: 0.9096 - val_loss: 0.4597 - val_acc: 0.8276\n",
      "Epoch 62/100\n",
      "323426/323426 [==============================] - 377s 1ms/step - loss: 0.2129 - acc: 0.9094 - val_loss: 0.4682 - val_acc: 0.8292\n",
      "Epoch 63/100\n",
      "323426/323426 [==============================] - 377s 1ms/step - loss: 0.2117 - acc: 0.9102 - val_loss: 0.4654 - val_acc: 0.8301\n",
      "Epoch 64/100\n",
      "323426/323426 [==============================] - 378s 1ms/step - loss: 0.2097 - acc: 0.9108 - val_loss: 0.4613 - val_acc: 0.8269\n",
      "Epoch 65/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.2090 - acc: 0.9109 - val_loss: 0.4596 - val_acc: 0.8295\n",
      "Epoch 66/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.2095 - acc: 0.9112 - val_loss: 0.4494 - val_acc: 0.8312\n",
      "Epoch 67/100\n",
      "323426/323426 [==============================] - 377s 1ms/step - loss: 0.2084 - acc: 0.9113 - val_loss: 0.4555 - val_acc: 0.8318\n",
      "Epoch 68/100\n",
      "323426/323426 [==============================] - 377s 1ms/step - loss: 0.2081 - acc: 0.9114 - val_loss: 0.4660 - val_acc: 0.8252\n",
      "Epoch 69/100\n",
      "323426/323426 [==============================] - 377s 1ms/step - loss: 0.2074 - acc: 0.9120 - val_loss: 0.4661 - val_acc: 0.8293\n",
      "Epoch 70/100\n",
      "323426/323426 [==============================] - 375s 1ms/step - loss: 0.2057 - acc: 0.9125 - val_loss: 0.4712 - val_acc: 0.8303\n",
      "Epoch 71/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.2045 - acc: 0.9139 - val_loss: 0.4738 - val_acc: 0.8301\n",
      "Epoch 72/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.2051 - acc: 0.9130 - val_loss: 0.4801 - val_acc: 0.8316\n",
      "Epoch 73/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.2041 - acc: 0.9137 - val_loss: 0.4724 - val_acc: 0.8316\n",
      "Epoch 74/100\n",
      "323426/323426 [==============================] - 374s 1ms/step - loss: 0.2039 - acc: 0.9135 - val_loss: 0.4666 - val_acc: 0.8307\n",
      "Epoch 75/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.2023 - acc: 0.9145 - val_loss: 0.4685 - val_acc: 0.8311\n",
      "Epoch 76/100\n",
      "323426/323426 [==============================] - 377s 1ms/step - loss: 0.2037 - acc: 0.9136 - val_loss: 0.4728 - val_acc: 0.8292\n",
      "Epoch 77/100\n",
      "323426/323426 [==============================] - 374s 1ms/step - loss: 0.2025 - acc: 0.9142 - val_loss: 0.4652 - val_acc: 0.8297\n",
      "Epoch 78/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.2023 - acc: 0.9146 - val_loss: 0.4664 - val_acc: 0.8299\n",
      "Epoch 79/100\n",
      "323426/323426 [==============================] - 377s 1ms/step - loss: 0.2018 - acc: 0.9146 - val_loss: 0.4730 - val_acc: 0.8293\n",
      "Epoch 80/100\n",
      "323426/323426 [==============================] - 375s 1ms/step - loss: 0.2016 - acc: 0.9143 - val_loss: 0.4701 - val_acc: 0.8294\n",
      "Epoch 81/100\n",
      "323426/323426 [==============================] - 377s 1ms/step - loss: 0.2000 - acc: 0.9156 - val_loss: 0.4777 - val_acc: 0.8304\n",
      "Epoch 82/100\n",
      "323426/323426 [==============================] - 375s 1ms/step - loss: 0.1988 - acc: 0.9161 - val_loss: 0.4757 - val_acc: 0.8285\n",
      "Epoch 83/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.1995 - acc: 0.9160 - val_loss: 0.4739 - val_acc: 0.8298\n",
      "Epoch 84/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.1992 - acc: 0.9158 - val_loss: 0.4731 - val_acc: 0.8286\n",
      "Epoch 85/100\n",
      "323426/323426 [==============================] - 377s 1ms/step - loss: 0.1988 - acc: 0.9163 - val_loss: 0.4783 - val_acc: 0.8289\n",
      "Epoch 86/100\n",
      "323426/323426 [==============================] - 375s 1ms/step - loss: 0.1985 - acc: 0.9162 - val_loss: 0.4771 - val_acc: 0.8300\n",
      "Epoch 87/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.1984 - acc: 0.9169 - val_loss: 0.4754 - val_acc: 0.8304\n",
      "Epoch 88/100\n",
      "323426/323426 [==============================] - 378s 1ms/step - loss: 0.1970 - acc: 0.9167 - val_loss: 0.4840 - val_acc: 0.8295\n",
      "Epoch 89/100\n",
      "323426/323426 [==============================] - 377s 1ms/step - loss: 0.1963 - acc: 0.9176 - val_loss: 0.4822 - val_acc: 0.8301\n",
      "Epoch 90/100\n",
      "323426/323426 [==============================] - 375s 1ms/step - loss: 0.1963 - acc: 0.9171 - val_loss: 0.4848 - val_acc: 0.8284\n",
      "Epoch 91/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.1962 - acc: 0.9172 - val_loss: 0.4750 - val_acc: 0.8311\n",
      "Epoch 92/100\n",
      "323426/323426 [==============================] - 375s 1ms/step - loss: 0.1954 - acc: 0.9180 - val_loss: 0.4828 - val_acc: 0.8314\n",
      "Epoch 93/100\n",
      "323426/323426 [==============================] - 377s 1ms/step - loss: 0.1956 - acc: 0.9180 - val_loss: 0.4761 - val_acc: 0.8311\n",
      "Epoch 94/100\n",
      "323426/323426 [==============================] - 375s 1ms/step - loss: 0.1955 - acc: 0.9169 - val_loss: 0.4792 - val_acc: 0.8308\n",
      "Epoch 95/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.1944 - acc: 0.9182 - val_loss: 0.4803 - val_acc: 0.8299\n",
      "Epoch 96/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.1941 - acc: 0.9181 - val_loss: 0.4825 - val_acc: 0.8285\n",
      "Epoch 97/100\n",
      "323426/323426 [==============================] - 377s 1ms/step - loss: 0.1932 - acc: 0.9185 - val_loss: 0.4862 - val_acc: 0.8288\n",
      "Epoch 98/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.1935 - acc: 0.9186 - val_loss: 0.4935 - val_acc: 0.8290\n",
      "Epoch 99/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.1925 - acc: 0.9197 - val_loss: 0.4757 - val_acc: 0.8279\n",
      "Epoch 100/100\n",
      "323426/323426 [==============================] - 376s 1ms/step - loss: 0.1935 - acc: 0.9183 - val_loss: 0.4764 - val_acc: 0.8302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efce0327a58>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime,time\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "callback_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='acc',\n",
    "        patience=5\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=MODEL_PATH,\n",
    "        monitor='val_acc',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "adadelta=Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "# model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['binary_accuracy'])\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "\n",
    "model.fit([x1_train, x2_train], y_train, epochs=100, batch_size=128, callbacks=callback_list, validation_data=([x1_valid, x2_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vec_pred = model.predict([x1_test, x2_test], batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def to_one_hot(prob):\n",
    "    prob_shape = prob.shape\n",
    "    y_pred = np.zeros(prob_shape)\n",
    "    \n",
    "    i = 0\n",
    "    for item in prob:\n",
    "        if item[0] < item[1]:\n",
    "            y_pred[i] = np.array([0, 1])\n",
    "        else:\n",
    "            y_pred[i] = np.array([1, 0])\n",
    "        i += 1\n",
    "    return y_pred\n",
    "\n",
    "y_pred = to_one_hot(y_vec_pred)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:81.23%\tRecall:81.15%\tF-score:81.19%\n"
     ]
    }
   ],
   "source": [
    "report = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "    \n",
    "def print_prf(result):\n",
    "    precision = str(round(result[0] * 100, 2)) + \"%\"\n",
    "    recall = str(round(result[1] * 100, 2)) + \"%\"\n",
    "    f1 = str(round(result[2] * 100, 2)) + \"%\"\n",
    "    \n",
    "    strResult=\"Precision:\"+precision+\"\\tRecall:\"+recall+\"\\tF-score:\"+f1\n",
    "    print(strResult)\n",
    "    \n",
    "print_prf(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40429/40429 [==============================] - 89s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.49180364691915407, 0.8246308343080071]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([x1_test, x2_test], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
