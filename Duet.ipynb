{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "from collections import Counter\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "import gc\n",
    "    \n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "seed(1000)\n",
    "from tensorflow.random import set_random_seed\n",
    "set_random_seed(1000)\n",
    "\n",
    "import pandas\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "# from keras.layers import Layer\n",
    "from keras.engine import Layer\n",
    "from keras.engine import InputSpec\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Input, Embedding, Conv1D, Concatenate, Flatten, Dropout, Dense, Bidirectional, Activation\n",
    "from keras.layers import MaxPooling1D, Conv2D, Flatten, Reshape, MaxPooling2D, LSTM, Lambda, Highway, Dot, Permute, Add\n",
    "from keras.models import Model, Sequential\n",
    "from keras.activations import softmax\n",
    "from keras.initializers import Constant, RandomNormal, RandomUniform\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns       \n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import  norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline \n",
    "\n",
    "#进行配置，使用30%的GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "# 设置session\n",
    "KTF.set_session(session )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文件路径\n",
    "ORIGINAL_FILE_PATH = \"../data/original_data/quora_duplicate_questions.tsv\"\n",
    "\n",
    "DATA_ALL_PATH = \"../data/all_data.txt\"\n",
    "\n",
    "DATA_TRAIN_PATH = \"../data/train.txt\"\n",
    "DATA_VALID_PATH = \"../data/valid.txt\"\n",
    "DATA_TEST_PATH = \"../data/test.txt\"\n",
    "\n",
    "MAPS_FILE_PATH = \"../data/maps.pkl\"\n",
    "\n",
    "# Glove文件\n",
    "WORD2VEC_PATH = \"../data/GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "# Model Path\n",
    "MODEL_PATH = \"../model/duet/duet-01.h5\"\n",
    "\n",
    "# 创建文件夹\n",
    "(file_path, file_name) = os.path.split(MODEL_PATH)\n",
    "if not os.path.exists(file_path):\n",
    "    os.mkdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(dict_path):\n",
    "    with open(dict_path, 'rb') as fr:\n",
    "        return pickle.load(fr)\n",
    "        \n",
    "def create_maps(dico):\n",
    "    item2id = dico\n",
    "    id2item = dict([val, key] for key, val in dico.items())\n",
    "    return item2id, id2item\n",
    "\n",
    "def read_file(fpath):\n",
    "    \"\"\"\n",
    "    读取文件，返回\n",
    "    \"\"\"\n",
    "    question1, question2, labels = [], [], []\n",
    "    with open(fpath, 'r', encoding=\"utf-8\") as fr:\n",
    "        for line in fr.readlines():\n",
    "            try:\n",
    "                q, k, l = line.strip().split('\\t')\n",
    "            except ValueError:\n",
    "                print(line)\n",
    "            question1.append(q)\n",
    "            question2.append(k)\n",
    "            labels.append(int(l))\n",
    "            \n",
    "    return question1, question2, labels\n",
    "\n",
    "def load_data(fpath, word_to_id, max_length=20):\n",
    "    \"\"\"\n",
    "    载入数据，并将其转化为id表示\n",
    "    \"\"\"\n",
    "    question1, question2, labels = read_file(fpath)\n",
    "    \n",
    "    q1_id, q2_id = [], []\n",
    "    for i in range(len(question1)):\n",
    "        q1= question1[i].split()\n",
    "        q2 = question2[i].split()\n",
    "        q1_id.append([word_to_id[x] for x in q1 if x in word_to_id])\n",
    "        q2_id.append([word_to_id[x] for x in q2 if x in word_to_id])\n",
    "\n",
    "    # 使用keras提供的pad_sequences来将文本pad为固定长度\n",
    "    x1_pad = keras.preprocessing.sequence.pad_sequences(q1_id, max_length, padding=\"post\")\n",
    "    x2_pad = keras.preprocessing.sequence.pad_sequences(q2_id, max_length, padding=\"post\")\n",
    "    labels = keras.utils.np_utils.to_categorical(labels)\n",
    "    \n",
    "    return x1_pad, x2_pad, labels\n",
    "    \n",
    "        \n",
    "def load_word2vec(embedding_path, word_index, embed_dim=300): \n",
    "    \"\"\"\n",
    "    载入预训练好的word2vec(Google-News)\n",
    "    \"\"\"\n",
    "    # 载入word2vec词向量\n",
    "    word2vec_dict = KeyedVectors.load_word2vec_format(embedding_path,binary=True)\n",
    "    \n",
    "    embedding_index = dict()\n",
    "    for word in word2vec_dict.wv.vocab:\n",
    "        embedding_index[word] = word2vec_dict.word_vec(word)\n",
    "    print('Load %s word vectors.' % len(embedding_index))\n",
    "    \n",
    "    all_embs = np.stack(list(embedding_index.values()))\n",
    "    # emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    emb_mean = np.mean(all_embs, axis=0)\n",
    "    vocab_size = len(word_index)\n",
    "    # 初始化权重\n",
    "    embedding_matrix = np.zeros((vocab_size+1, embed_dim))\n",
    "    gc.collect()\n",
    "    # 对权重矩阵进行赋值，未找到时用词向量平均值填充\n",
    "    for word, i in word_index.items():\n",
    "        if i < vocab_size + 1:\n",
    "            embedding_vector = embedding_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "            else:\n",
    "                embedding_matrix[i] = emb_mean\n",
    "    del embedding_index\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 3000000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "dico = load_dict(MAPS_FILE_PATH)\n",
    "word_index, index_word = create_maps(dico)\n",
    "\n",
    "# 准备数据\n",
    "x1_train, x2_train, y_train = load_data(DATA_TRAIN_PATH, word_index)\n",
    "x1_valid, x2_valid, y_valid = load_data(DATA_VALID_PATH, word_index)\n",
    "x1_test, x2_test, y_test = load_data(DATA_TEST_PATH, word_index)\n",
    "\n",
    "embedding_matrix = load_word2vec(WORD2VEC_PATH, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(dico)\n",
    "\n",
    "class Settings(object):\n",
    "    text1_maxlen = 20\n",
    "    text2_maxlen = 20\n",
    "    embedding_dim = 300\n",
    "    vocab_size = vocab_size\n",
    "    \n",
    "    lm_kernel_count = 32\n",
    "    lm_hidden_sizes = [50]\n",
    "    dm_kermel_count = 32\n",
    "    dm_kernel_size = 3\n",
    "    dm_q_hidden_size = 32\n",
    "    dm_d_mpool = 3\n",
    "    dm_hidden_sizes = [50]\n",
    "    lm_dropout_rate = 0.2\n",
    "    dm_dropout_rate = 0.2\n",
    "    \n",
    "    dropout_rate = 0.2\n",
    "    \n",
    "settings = Settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "query (InputLayer)              (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "doc (InputLayer)                (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 20, 300)      25293600    query[0][0]                      \n",
      "                                                                 doc[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 32)       28832       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 20, 32)       0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 32)       28832       embedding_4[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 1, 32)        0           dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 20, 32)       0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 32)           0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 6, 32)        0           dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 20, 20)       0           query[0][0]                      \n",
      "                                                                 doc[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 32)           1056        reshape_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 6, 32)        1056        max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 32)       12832       lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 1, 32)        0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 6, 32)        0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 20, 32)       0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 6, 32)        0           lambda_8[0][0]                   \n",
      "                                                                 dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 640)          0           dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 192)          0           lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 50)           32050       reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 50)           9650        reshape_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 50)           0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 50)           0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1)            51          dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1)            51          dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 1)            0           dense_12[0][0]                   \n",
      "                                                                 dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 2)            4           add_2[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 25,408,014\n",
      "Trainable params: 114,414\n",
      "Non-trainable params: 25,293,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 定义网络\n",
    "def xor_match(x):\n",
    "    t1 = x[0]\n",
    "    t2 = x[1]\n",
    "    t1_shape = t1.get_shape()\n",
    "    t2_shape = t2.get_shape()\n",
    "    t1_expand = K.tf.stack([t1] * t2_shape[1], 2)\n",
    "    t2_expand = K.tf.stack([t2] * t1_shape[1], 1)\n",
    "    out_bool = K.tf.equal(t1_expand, t2_expand)\n",
    "    out = K.tf.cast(out_bool, K.tf.float32)\n",
    "    return out\n",
    "def hadamard_dot(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    out = x1 * x2\n",
    "    return out\n",
    "\n",
    "query = Input(name='query', shape=(settings.text1_maxlen,))\n",
    "doc = Input(name='doc', shape=(settings.text2_maxlen, ))\n",
    "\n",
    "\n",
    "embedding = Embedding(settings.vocab_size + 1,\n",
    "                      settings.embedding_dim,\n",
    "                      weights=[embedding_matrix],\n",
    "                      input_length=settings.text1_maxlen,\n",
    "                      trainable=False)\n",
    "\n",
    "q_embed = embedding(query)\n",
    "d_embed = embedding(doc)\n",
    "\n",
    "lm_xor = Lambda(xor_match)([query, doc])\n",
    "lm_conv = Conv1D(settings.lm_kernel_count, settings.text2_maxlen, padding='same', activation='tanh')(lm_xor)\n",
    "lm_conv = Dropout(settings.lm_dropout_rate)(lm_conv)\n",
    "lm_feat = Reshape((-1,))(lm_conv)\n",
    "for hidden_size in settings.lm_hidden_sizes:\n",
    "    lm_feat = Dense(hidden_size, activation='tanh')(lm_feat)\n",
    "\n",
    "lm_drop = Dropout(settings.lm_dropout_rate)(lm_feat)\n",
    "lm_score = Dense(1)(lm_drop)\n",
    "dm_q_conv = Conv1D(settings.dm_kermel_count, settings.dm_kernel_size, padding='same', activation='tanh')(q_embed)\n",
    "dm_q_conv = Dropout(settings.dm_dropout_rate)(dm_q_conv)\n",
    "dm_q_mp = MaxPooling1D(pool_size = settings.text1_maxlen)(dm_q_conv)        \n",
    "dm_q_rep = Reshape((-1,))(dm_q_mp)\n",
    "dm_q_rep = Dense(settings.dm_q_hidden_size)(dm_q_rep)\n",
    "dm_q_rep = Lambda(lambda x: tf.expand_dims(x, 1))(dm_q_rep)    \n",
    "dm_d_conv1 = Conv1D(settings.dm_kermel_count, settings.dm_kernel_size, padding='same', activation='tanh')(d_embed)        \n",
    "dm_d_conv1 = Dropout(settings.dm_dropout_rate)(dm_d_conv1)\n",
    "dm_d_mp = MaxPooling1D(pool_size = settings.dm_d_mpool)(dm_d_conv1)      \n",
    "dm_d_conv2 = Conv1D(settings.dm_kermel_count, 1, padding='same', activation='tanh')(dm_d_mp)\n",
    "dm_d_conv2 = Dropout(settings.dm_dropout_rate)(dm_d_conv2)\n",
    "h_dot = Lambda(hadamard_dot)([dm_q_rep, dm_d_conv2])        \n",
    "dm_feat = Reshape((-1,))(h_dot)\n",
    "for hidden_size in settings.dm_hidden_sizes:\n",
    "    dm_feat = Dense(hidden_size)(dm_feat)\n",
    "        \n",
    "dm_feat_drop = Dropout(settings.dm_dropout_rate)(dm_feat)        \n",
    "       \n",
    "dm_score = Dense(1)(dm_feat_drop)\n",
    "out_ = Add()([lm_score, dm_score])\n",
    "\n",
    "out_ = Dense(2, activation='softmax')(out_)\n",
    "\n",
    "model = Model(inputs=[query, doc], outputs=out_)\n",
    "model.summary()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 323426 samples, validate on 40428 samples\n",
      "Epoch 1/100\n",
      "323426/323426 [==============================] - 31s 95us/step - loss: 0.5010 - acc: 0.7413 - val_loss: 0.4572 - val_acc: 0.7696\n",
      "Epoch 2/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.4502 - acc: 0.7753 - val_loss: 0.4364 - val_acc: 0.7837\n",
      "Epoch 3/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.4304 - acc: 0.7887 - val_loss: 0.4322 - val_acc: 0.7911\n",
      "Epoch 4/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.4168 - acc: 0.7972 - val_loss: 0.4169 - val_acc: 0.7975\n",
      "Epoch 5/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.4062 - acc: 0.8037 - val_loss: 0.4124 - val_acc: 0.8006\n",
      "Epoch 6/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3982 - acc: 0.8085 - val_loss: 0.4351 - val_acc: 0.7899\n",
      "Epoch 7/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3907 - acc: 0.8129 - val_loss: 0.4188 - val_acc: 0.8002\n",
      "Epoch 8/100\n",
      "323426/323426 [==============================] - 28s 88us/step - loss: 0.3859 - acc: 0.8172 - val_loss: 0.4106 - val_acc: 0.8026\n",
      "Epoch 9/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3798 - acc: 0.8197 - val_loss: 0.4243 - val_acc: 0.7957\n",
      "Epoch 10/100\n",
      "323426/323426 [==============================] - 28s 88us/step - loss: 0.3745 - acc: 0.8231 - val_loss: 0.4095 - val_acc: 0.8057\n",
      "Epoch 11/100\n",
      "323426/323426 [==============================] - 28s 86us/step - loss: 0.3706 - acc: 0.8249 - val_loss: 0.4415 - val_acc: 0.7932\n",
      "Epoch 12/100\n",
      "323426/323426 [==============================] - 28s 86us/step - loss: 0.3663 - acc: 0.8272 - val_loss: 0.4127 - val_acc: 0.8053\n",
      "Epoch 13/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3641 - acc: 0.8289 - val_loss: 0.4115 - val_acc: 0.8032\n",
      "Epoch 14/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3606 - acc: 0.8309 - val_loss: 0.4088 - val_acc: 0.8060\n",
      "Epoch 15/100\n",
      "323426/323426 [==============================] - 28s 86us/step - loss: 0.3572 - acc: 0.8331 - val_loss: 0.4192 - val_acc: 0.8035\n",
      "Epoch 16/100\n",
      "323426/323426 [==============================] - 28s 88us/step - loss: 0.3549 - acc: 0.8342 - val_loss: 0.4054 - val_acc: 0.8093\n",
      "Epoch 17/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3519 - acc: 0.8362 - val_loss: 0.4379 - val_acc: 0.7977\n",
      "Epoch 18/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3504 - acc: 0.8373 - val_loss: 0.4167 - val_acc: 0.8066\n",
      "Epoch 19/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3479 - acc: 0.8386 - val_loss: 0.4124 - val_acc: 0.8060\n",
      "Epoch 20/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3447 - acc: 0.8407 - val_loss: 0.4317 - val_acc: 0.8039\n",
      "Epoch 21/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3445 - acc: 0.8401 - val_loss: 0.4285 - val_acc: 0.8002\n",
      "Epoch 22/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3425 - acc: 0.8417 - val_loss: 0.4267 - val_acc: 0.8020\n",
      "Epoch 23/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3395 - acc: 0.8431 - val_loss: 0.4289 - val_acc: 0.8024\n",
      "Epoch 24/100\n",
      "323426/323426 [==============================] - 28s 86us/step - loss: 0.3383 - acc: 0.8434 - val_loss: 0.4416 - val_acc: 0.7992\n",
      "Epoch 25/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3371 - acc: 0.8444 - val_loss: 0.4480 - val_acc: 0.7962\n",
      "Epoch 26/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3353 - acc: 0.8456 - val_loss: 0.4339 - val_acc: 0.8014\n",
      "Epoch 27/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3333 - acc: 0.8473 - val_loss: 0.4447 - val_acc: 0.7981\n",
      "Epoch 28/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3320 - acc: 0.8472 - val_loss: 0.4227 - val_acc: 0.8044\n",
      "Epoch 29/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3314 - acc: 0.8475 - val_loss: 0.4423 - val_acc: 0.7994\n",
      "Epoch 30/100\n",
      "323426/323426 [==============================] - 29s 89us/step - loss: 0.3295 - acc: 0.8488 - val_loss: 0.4317 - val_acc: 0.8048\n",
      "Epoch 31/100\n",
      "323426/323426 [==============================] - 28s 88us/step - loss: 0.3287 - acc: 0.8492 - val_loss: 0.4199 - val_acc: 0.8092\n",
      "Epoch 32/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3280 - acc: 0.8497 - val_loss: 0.4389 - val_acc: 0.8023\n",
      "Epoch 33/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3267 - acc: 0.8500 - val_loss: 0.4197 - val_acc: 0.8106\n",
      "Epoch 34/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3244 - acc: 0.8514 - val_loss: 0.4204 - val_acc: 0.8065\n",
      "Epoch 35/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3244 - acc: 0.8517 - val_loss: 0.4308 - val_acc: 0.8034\n",
      "Epoch 36/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3231 - acc: 0.8522 - val_loss: 0.4325 - val_acc: 0.8042\n",
      "Epoch 37/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3217 - acc: 0.8533 - val_loss: 0.4244 - val_acc: 0.8052\n",
      "Epoch 38/100\n",
      "323426/323426 [==============================] - 28s 86us/step - loss: 0.3208 - acc: 0.8532 - val_loss: 0.4117 - val_acc: 0.8121\n",
      "Epoch 39/100\n",
      "323426/323426 [==============================] - 28s 88us/step - loss: 0.3200 - acc: 0.8543 - val_loss: 0.4610 - val_acc: 0.7930\n",
      "Epoch 40/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3189 - acc: 0.8542 - val_loss: 0.4100 - val_acc: 0.8130\n",
      "Epoch 41/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3174 - acc: 0.8554 - val_loss: 0.4376 - val_acc: 0.8042\n",
      "Epoch 42/100\n",
      "323426/323426 [==============================] - 28s 86us/step - loss: 0.3169 - acc: 0.8558 - val_loss: 0.4320 - val_acc: 0.8048\n",
      "Epoch 43/100\n",
      "323426/323426 [==============================] - 28s 86us/step - loss: 0.3164 - acc: 0.8560 - val_loss: 0.4524 - val_acc: 0.7994\n",
      "Epoch 44/100\n",
      "323426/323426 [==============================] - 28s 86us/step - loss: 0.3149 - acc: 0.8565 - val_loss: 0.4363 - val_acc: 0.8024\n",
      "Epoch 45/100\n",
      "323426/323426 [==============================] - 28s 86us/step - loss: 0.3140 - acc: 0.8572 - val_loss: 0.4467 - val_acc: 0.8041\n",
      "Epoch 46/100\n",
      "323426/323426 [==============================] - 28s 86us/step - loss: 0.3134 - acc: 0.8568 - val_loss: 0.4328 - val_acc: 0.7999\n",
      "Epoch 47/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3131 - acc: 0.8576 - val_loss: 0.4513 - val_acc: 0.8013\n",
      "Epoch 48/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3119 - acc: 0.8586 - val_loss: 0.4495 - val_acc: 0.8009\n",
      "Epoch 49/100\n",
      "323426/323426 [==============================] - 28s 88us/step - loss: 0.3110 - acc: 0.8596 - val_loss: 0.4307 - val_acc: 0.8039\n",
      "Epoch 50/100\n",
      "323426/323426 [==============================] - 28s 86us/step - loss: 0.3113 - acc: 0.8585 - val_loss: 0.4540 - val_acc: 0.7990\n",
      "Epoch 51/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3102 - acc: 0.8587 - val_loss: 0.4178 - val_acc: 0.8098\n",
      "Epoch 52/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3085 - acc: 0.8595 - val_loss: 0.4144 - val_acc: 0.8116\n",
      "Epoch 53/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3081 - acc: 0.8604 - val_loss: 0.4597 - val_acc: 0.8020\n",
      "Epoch 54/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3076 - acc: 0.8605 - val_loss: 0.4538 - val_acc: 0.8005\n",
      "Epoch 55/100\n",
      "323426/323426 [==============================] - 28s 88us/step - loss: 0.3070 - acc: 0.8610 - val_loss: 0.4486 - val_acc: 0.7999\n",
      "Epoch 56/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3063 - acc: 0.8611 - val_loss: 0.4342 - val_acc: 0.8039\n",
      "Epoch 57/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3057 - acc: 0.8613 - val_loss: 0.4402 - val_acc: 0.8051\n",
      "Epoch 58/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3052 - acc: 0.8620 - val_loss: 0.4587 - val_acc: 0.7958\n",
      "Epoch 59/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3042 - acc: 0.8631 - val_loss: 0.4542 - val_acc: 0.8004\n",
      "Epoch 60/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3040 - acc: 0.8627 - val_loss: 0.4325 - val_acc: 0.8071\n",
      "Epoch 61/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3041 - acc: 0.8627 - val_loss: 0.4355 - val_acc: 0.8038\n",
      "Epoch 62/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3030 - acc: 0.8634 - val_loss: 0.4439 - val_acc: 0.8038\n",
      "Epoch 63/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3028 - acc: 0.8634 - val_loss: 0.4311 - val_acc: 0.8060\n",
      "Epoch 64/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3014 - acc: 0.8638 - val_loss: 0.4216 - val_acc: 0.8099\n",
      "Epoch 65/100\n",
      "323426/323426 [==============================] - 28s 86us/step - loss: 0.3018 - acc: 0.8632 - val_loss: 0.4322 - val_acc: 0.8058\n",
      "Epoch 66/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.3015 - acc: 0.8638 - val_loss: 0.4379 - val_acc: 0.8054\n",
      "Epoch 67/100\n",
      "323426/323426 [==============================] - 28s 86us/step - loss: 0.2992 - acc: 0.8651 - val_loss: 0.4522 - val_acc: 0.8021\n",
      "Epoch 68/100\n",
      "323426/323426 [==============================] - 28s 88us/step - loss: 0.2996 - acc: 0.8652 - val_loss: 0.4609 - val_acc: 0.7999\n",
      "Epoch 69/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.2996 - acc: 0.8644 - val_loss: 0.4547 - val_acc: 0.8024\n",
      "Epoch 70/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.2994 - acc: 0.8650 - val_loss: 0.4413 - val_acc: 0.8006\n",
      "Epoch 71/100\n",
      "323426/323426 [==============================] - 28s 88us/step - loss: 0.2990 - acc: 0.8651 - val_loss: 0.4530 - val_acc: 0.8050\n",
      "Epoch 72/100\n",
      "323426/323426 [==============================] - 28s 86us/step - loss: 0.2981 - acc: 0.8660 - val_loss: 0.4314 - val_acc: 0.8104\n",
      "Epoch 73/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.2975 - acc: 0.8655 - val_loss: 0.4609 - val_acc: 0.7984\n",
      "Epoch 74/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.2965 - acc: 0.8669 - val_loss: 0.4258 - val_acc: 0.8082\n",
      "Epoch 75/100\n",
      "323426/323426 [==============================] - 29s 88us/step - loss: 0.2955 - acc: 0.8667 - val_loss: 0.4474 - val_acc: 0.8058\n",
      "Epoch 76/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.2965 - acc: 0.8668 - val_loss: 0.4286 - val_acc: 0.8106\n",
      "Epoch 77/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.2958 - acc: 0.8671 - val_loss: 0.4747 - val_acc: 0.7987\n",
      "Epoch 78/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.2952 - acc: 0.8671 - val_loss: 0.4696 - val_acc: 0.7985\n",
      "Epoch 79/100\n",
      "323426/323426 [==============================] - 28s 88us/step - loss: 0.2944 - acc: 0.8672 - val_loss: 0.4231 - val_acc: 0.8087\n",
      "Epoch 80/100\n",
      "323426/323426 [==============================] - 28s 88us/step - loss: 0.2938 - acc: 0.8685 - val_loss: 0.4389 - val_acc: 0.8058\n",
      "Epoch 81/100\n",
      "323426/323426 [==============================] - 28s 88us/step - loss: 0.2937 - acc: 0.8684 - val_loss: 0.4405 - val_acc: 0.8057\n",
      "Epoch 82/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.2936 - acc: 0.8682 - val_loss: 0.4373 - val_acc: 0.8075\n",
      "Epoch 83/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.2928 - acc: 0.8681 - val_loss: 0.4411 - val_acc: 0.8057\n",
      "Epoch 84/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.2924 - acc: 0.8686 - val_loss: 0.4163 - val_acc: 0.8093\n",
      "Epoch 85/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.2917 - acc: 0.8690 - val_loss: 0.4620 - val_acc: 0.8010\n",
      "Epoch 86/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.2919 - acc: 0.8685 - val_loss: 0.4266 - val_acc: 0.8090\n",
      "Epoch 87/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.2910 - acc: 0.8686 - val_loss: 0.4435 - val_acc: 0.8068\n",
      "Epoch 88/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.2902 - acc: 0.8702 - val_loss: 0.4631 - val_acc: 0.8016\n",
      "Epoch 89/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.2905 - acc: 0.8695 - val_loss: 0.4929 - val_acc: 0.7936\n",
      "Epoch 90/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.2908 - acc: 0.8694 - val_loss: 0.4466 - val_acc: 0.8048\n",
      "Epoch 91/100\n",
      "323426/323426 [==============================] - 27s 84us/step - loss: 0.2895 - acc: 0.8705 - val_loss: 0.4391 - val_acc: 0.8049\n",
      "Epoch 92/100\n",
      "323426/323426 [==============================] - 28s 86us/step - loss: 0.2904 - acc: 0.8695 - val_loss: 0.4630 - val_acc: 0.7990\n",
      "Epoch 93/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.2882 - acc: 0.8710 - val_loss: 0.4436 - val_acc: 0.8041\n",
      "Epoch 94/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.2888 - acc: 0.8705 - val_loss: 0.4734 - val_acc: 0.7992\n",
      "Epoch 95/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.2872 - acc: 0.8715 - val_loss: 0.4365 - val_acc: 0.8037\n",
      "Epoch 96/100\n",
      "323426/323426 [==============================] - 28s 87us/step - loss: 0.2878 - acc: 0.8712 - val_loss: 0.4442 - val_acc: 0.8033\n",
      "Epoch 97/100\n",
      "323426/323426 [==============================] - 28s 86us/step - loss: 0.2876 - acc: 0.8710 - val_loss: 0.4776 - val_acc: 0.7992\n",
      "Epoch 98/100\n",
      "323426/323426 [==============================] - 28s 88us/step - loss: 0.2874 - acc: 0.8712 - val_loss: 0.4637 - val_acc: 0.8030\n",
      "Epoch 99/100\n",
      "323426/323426 [==============================] - 28s 88us/step - loss: 0.2878 - acc: 0.8710 - val_loss: 0.4307 - val_acc: 0.8075\n",
      "Epoch 100/100\n",
      "323426/323426 [==============================] - 28s 86us/step - loss: 0.2865 - acc: 0.8718 - val_loss: 0.4649 - val_acc: 0.8019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f83a01efe10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime,time\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "callback_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='acc',\n",
    "        patience=5\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=MODEL_PATH,\n",
    "        monitor='val_acc',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "adadelta=Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "# model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['binary_accuracy'])\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "\n",
    "model.fit([x1_train, x2_train], y_train, epochs=100, batch_size=128, callbacks=callback_list, validation_data=([x1_valid, x2_valid], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vec_pred = model.predict([x1_test, x2_test], batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def to_one_hot(prob):\n",
    "    prob_shape = prob.shape\n",
    "    y_pred = np.zeros(prob_shape)\n",
    "    \n",
    "    i = 0\n",
    "    for item in prob:\n",
    "        if item[0] < item[1]:\n",
    "            y_pred[i] = np.array([0, 1])\n",
    "        else:\n",
    "            y_pred[i] = np.array([1, 0])\n",
    "        i += 1\n",
    "    return y_pred\n",
    "\n",
    "y_pred = to_one_hot(y_vec_pred)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:78.46%\tRecall:79.99%\tF-score:78.86%\n"
     ]
    }
   ],
   "source": [
    "report = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "    \n",
    "def print_prf(result):\n",
    "    precision = str(round(result[0] * 100, 2)) + \"%\"\n",
    "    recall = str(round(result[1] * 100, 2)) + \"%\"\n",
    "    f1 = str(round(result[2] * 100, 2)) + \"%\"\n",
    "    \n",
    "    strResult=\"Precision:\"+precision+\"\\tRecall:\"+recall+\"\\tF-score:\"+f1\n",
    "    print(strResult)\n",
    "    \n",
    "print_prf(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40429/40429 [==============================] - 4s 87us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.47495094006041916, 0.7964085186420345]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([x1_test, x2_test], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
